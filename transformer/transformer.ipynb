{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmoA9HyvlvhyUuHiAZrD8Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/won-hj/deep_learning_study/blob/main/transformer/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ],
      "metadata": {
        "id": "2yBi8Ce9-uy4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDO0pbOS-r5A"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural machine translation with a Transformer and Keras"
      ],
      "metadata": {
        "id": "ymwiSdHJ-ui4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial demonstrates how to create and train a [sequence-to-sequence](https://developers.google.com/machine-learning/glossary#sequence-to-sequence-task) [Transformer](https://developers.google.com/machine-learning/glossary#Transformer) model to translate [Portuguese into English](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en). The Transformer was originally proposed in [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017).\n",
        "\n",
        "Transformers are deep neural networks that replace CNNs and RNNs with [self-attention](https://developers.google.com/machine-learning/glossary#self-attention). Self-attention allows Transformers to easily transmit information across the input sequences.\n",
        "\n",
        "As explained in the [Google AI Blog post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html):\n",
        "\n",
        "> Neural networks for machine translation typically contain an encoder reading the input sentence and generating a representation of it. A decoder then generates the output sentence word by word while consulting the representation generated by the encoder. The Transformer starts by generating initial representations, or embeddings, for each word... Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations."
      ],
      "metadata": {
        "id": "obGbIrIr-01Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif\" alt=\"Applying the Transformer to machine translation\">\n",
        "\n",
        "Figure 1: Applying the Transformer to machine translation. Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).\n"
      ],
      "metadata": {
        "id": "HZCGVeUK-03Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a lot to digest, the goal of this tutorial is to break it down into easy to understand parts. In this tutorial you will:\n",
        "\n",
        "- Prepare the data.\n",
        "- Implement necessary components:\n",
        "  - Positional embeddings.\n",
        "  - Attention layers.\n",
        "  - The encoder and decoder.\n",
        "- Build & train the Transformer.\n",
        "- Generate translations.\n",
        "- Export the model."
      ],
      "metadata": {
        "id": "fcrwcQRI-05f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the most out of this tutorial, it helps if you know about [the basics of text generation](./text_generation.ipynb) and attention mechanisms.\n",
        "\n",
        "A Transformer is a sequence-to-sequence encoder-decoder model similar to the model in the [NMT with attention tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention).\n",
        "A single-layer Transformer takes a little more code to write, but is almost identical to that encoder-decoder RNN model. The only difference is that the RNN layers are replaced with self-attention layers.\n",
        "This tutorial builds a 4-layer Transformer which is larger and more powerful, but not fundamentally more complex."
      ],
      "metadata": {
        "id": "HrDYDn6e-07j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>The <a href=https://www.tensorflow.org/text/tutorials/nmt_with_attention>RNN+Attention model</a></th>\n",
        "  <th>A 1-layer transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=411 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-words.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "aUkVhIMu-09j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the model in this notebook, you will be able to input a Portuguese sentence and return the English translation.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png\" alt=\"Attention heatmap\">\n",
        "\n",
        "Figure 2: Visualized attention weights that you can generate at the end of this tutorial."
      ],
      "metadata": {
        "id": "fEy7B-oS-0_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Transformers are significant\n",
        "\n",
        "- Transformers excel at modeling sequential data, such as natural language.\n",
        "- Unlike [recurrent neural networks (RNNs)](./text_generation.ipynb), Transformers are parallelizable. This makes them efficient on hardware like GPUs and TPUs. The main reasons is that Transformers replaced recurrence with attention, and computations can happen simultaneously. Layer outputs can be computed in parallel, instead of a series like an RNN.\n",
        "- Unlike [RNNs](https://www.tensorflow.org/guide/keras/rnn) (such as [seq2seq, 2014](https://arxiv.org/abs/1409.3215)) or [convolutional neural networks (CNNs)](https://www.tensorflow.org/tutorials/images/cnn) (for example, [ByteNet](https://arxiv.org/abs/1610.10099)), Transformers are able to capture distant or long-range contexts and dependencies in the data between distant positions in the input or output sequences. Thus, longer connections can be learned. Attention allows each location to have access to the entire input at each layer, while in RNNs and CNNs, the information needs to pass through many processing steps to move a long distance, which makes it harder to learn.\n",
        "- Transformers make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, [StarCraft units](https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii)).\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/encoder_self_attention_distribution.png\" width=\"800\" alt=\"Encoder self-attention distribution for the word it from the 5th to the 6th layer of a Transformer trained on English-to-French translation\">\n",
        "\n",
        "Figure 3: The encoder self-attention distribution for the word “it” from the 5th to the 6th layer of a Transformer trained on English-to-French translation (one of eight attention heads). Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)."
      ],
      "metadata": {
        "id": "iF_0VGY5-1Be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "DBC3pGW3-1Dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin by installing [TensorFlow Datasets](https://tensorflow.org/datasets) for loading the dataset and [TensorFlow Text](https://www.tensorflow.org/text) for text preprocessing:"
      ],
      "metadata": {
        "id": "FYW6p-lz-1F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install the most re version of TensorFlow to use the improved\n",
        "#masking support for `tf.keras.layers.MultiHeadAttention`\n",
        "!atp install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install protobuf ~= 3.20.3\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ],
      "metadata": {
        "id": "EcZezGh2B1Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary modeuls:"
      ],
      "metadata": {
        "id": "SiZBVBUL-1IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text"
      ],
      "metadata": {
        "id": "O_wiOS2XCZGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data handling\n",
        "\n",
        "This section downloads the dataset and the subword tokenizer, from [this tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer), then wraps it all up in a `tf.data.Dataset` for training.\n",
        "\n",
        " <section class=\"expandable tfo-display-only-on-site\">\n",
        " <button type=\"button\" class=\"button-red button expand-control\">Toggle section</button>\n"
      ],
      "metadata": {
        "id": "Spb0Y4N6-1KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the dataset"
      ],
      "metadata": {
        "id": "oKhRnkC2-1Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use TensorFlow Datasets to load the [Portuguese-English translation dataset](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en)D Talks Open Translation Project. This dataset contains approximately 52,000 training, 1,200 validation and 1,800 test examples."
      ],
      "metadata": {
        "id": "4cKTmJOl-1O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
        "                               with_info=True,\n",
        "                               as_supervised=True)\n",
        "\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "id": "q6W53xLaC8Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `tf.data.Dataset` object returned by TensorFlow Datasets yields pairs of text examples:"
      ],
      "metadata": {
        "id": "iBhaYWMT-1RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  pring('> Examples in Portuguese:')\n",
        "  for pt in pt_examples.numpy():\n",
        "    print(pt.decode('utf-8'))\n",
        "  print()\n",
        "\n",
        "  print('> Examples in English:')\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ],
      "metadata": {
        "id": "NXB3CCDeDPmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the tokenizer"
      ],
      "metadata": {
        "id": "BhGfdRgH-1TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have loaded the dataset, you need to tokenize the text, so that each element is represented as a [token](https://developers.google.com/machine-learning/glossary#token) or token ID (a numeric representation).\n",
        "\n",
        "Tokenization is the process of breaking up text, into \"tokens\". Depending on the tokenizer, these tokens can represent sentence-pieces, words, subwords, or characters. To learn more about tokenization, visit [this guide](https://www.tensorflow.org/text/guide/tokenizers)."
      ],
      "metadata": {
        "id": "KHdmptwY-1VZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial uses the tokenizers built in the [subword tokenizer](https://www.tensorflow.org/text/guide/subwords_tokenizer) tutorial. That tutorial optimizes two `text.BertTokenizer` objects (one for English, one for Portuguese) for **this dataset** and exports them in a TensorFlow `saved_model` format.\n",
        "\n",
        "> Note: This is different from the [original paper](https://arxiv.org/pdf/1706.03762.pdf), section 5.1, where they used a single byte-pair tokenizer for both the source and target with a vocabulary-size of 37000.\n",
        "\n",
        "Download, extract, and import the `saved_model`:"
      ],
      "metadata": {
        "id": "dSnE36Jw-1Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.keras.utils.get_file(\n",
        "    f'{model_name}.zip',\n",
        "    origin=f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "    cache_dir='.', cache_subdir='', extract=True\n",
        ")"
      ],
      "metadata": {
        "id": "zXLrhIfcEI_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers = tf.saved_model.load(model_name)"
      ],
      "metadata": {
        "id": "Ds2O-iuJEhj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `tf.saved_model` contains two text tokenizers, one for English and one for Portuguese. Both have the same methods:"
      ],
      "metadata": {
        "id": "izHiErEjDsjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[item for item in dif(tokenizers.en) if not item.startwith('_')]"
      ],
      "metadata": {
        "id": "GtbPESXuEvVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `tokenize` method converts a batch of strings to a padded-batch of token IDs. This method splits punctuation, lowercases and unicode-normalizes the input before tokenizing. That standardization is not visible here because the input data is already standardized."
      ],
      "metadata": {
        "id": "TfuJ-rGlDslS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('> This is a batch of strings:')\n",
        "for en in en_examples.numpy():\n",
        "  print(en.decode('utf-8'))"
      ],
      "metadata": {
        "id": "tBVzDVPDE-6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizers.en.tokenize(en_examples)\n",
        "\n",
        "print('> This is a padded-batch of token IDs:')\n",
        "for row in encoded.to_list():\n",
        "  print(row)"
      ],
      "metadata": {
        "id": "ZEfDmhkgFFXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `detokenize` method attempts to convert these token IDs back to human-readable text:"
      ],
      "metadata": {
        "id": "QEksIOgZDsnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "round_trip = tokenizers.en.detokenize(encoded)\n",
        "\n",
        "print('> This is human-readable text:')\n",
        "for line in round_trip.numpy():\n",
        "  print(line.decode('utf-8'))"
      ],
      "metadata": {
        "id": "y3d-5Ia9Fjbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lower level `lookup` method converts from token-IDs to token text:"
      ],
      "metadata": {
        "id": "XmXshE30Dspy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('> This is the text split into tokens:')\n",
        "tokens = tokenizers.en.lookup(encoded)\n",
        "tokens"
      ],
      "metadata": {
        "id": "XIJLGOKYFhg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output demonstrates the \"subword\" aspect of the subword tokenization.\n",
        "\n",
        "For example, the word `'searchability'` is decomposed into `'search'` and `'##ability'`, and the word `'serendipity'` into `'s'`, `'##ere'`, `'##nd'`, `'##ip'` and `'##ity'`.\n",
        "\n",
        "Note that the tokenized text includes `'[START]'` and `'[END]'` tokens."
      ],
      "metadata": {
        "id": "jPsWTlyzDsr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of tokens per example in the dataset is as follows:"
      ],
      "metadata": {
        "id": "eFphKw8UDsvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "\n",
        "for pt_examples, en_examples in train_examples.batch(1024):\n",
        "  pt_tokens = tokenizers.pt.tokenize(pt_examples)\n",
        "  lengths.append(pt_tokens.row_lengths())\n",
        "\n",
        "  en_tokens = tokenizers.en.tokenize(en_examples)\n",
        "  lengths.append(en_tokens.row_lengths())\n",
        "\n",
        "  print('.', end='', flush=True)"
      ],
      "metadata": {
        "id": "YMDa-fNpGHKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ],
      "metadata": {
        "id": "pBF82uYzGhFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up a data pipeline with `tf.data`"
      ],
      "metadata": {
        "id": "YBX2m2PDDsxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function takes batches of text as input, and converts them to a format suitable for training.\n",
        "\n",
        "1. It tokenizes them into ragged batches.\n",
        "2. It trims each to be no longer than `MAX_TOKENS`.\n",
        "3. It splits the target (English) tokens into inputs and labels. These are shifted by one step so that at each input location the `label` is the id of the next token.\n",
        "4. It converts the `RaggedTensor`s to padded dense `Tensor`s.\n",
        "5. It returns an `(inputs, labels)` pair.\n"
      ],
      "metadata": {
        "id": "NdD9cAAtDszk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKEN = 128\n",
        "def prepare_batch(pt, en):\n",
        "  pt = tokenizers.pt.tokenize(pt)   #Output is ragged\n",
        "  pt = pt[:, :MAX_TOKENS]   #Trim the MAX_TOKENS\n",
        "  pt = pt.to_tensor()   #Convert to 0-padded dense tensor\n",
        "\n",
        "  en = tokenizers.en.tokenize(en)\n",
        "  en = en[:, :(MAX_TOKENS+1)]\n",
        "  en_inputs = en[:, :-1].to_tensor()  #drop the [END] tokens\n",
        "  en_labels = en[:, 1:].to_tensor()  #drop the [START] tokens\n",
        "\n",
        "  return (pt, en_inputs), en_labels"
      ],
      "metadata": {
        "id": "ddkUjHjCG_D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below converts a dataset of text examples into data of batches for training.\n",
        "\n",
        "1. It tokenizes the text, and filters out the sequences that are too long.\n",
        "   (The `batch`/`unbatch` is included because the tokenizer is much more efficient on large batches).\n",
        "2. The `cache` method ensures that that work is only executed once.\n",
        "3. Then `shuffle` and, `dense_to_ragged_batch` randomize the order and assemble batches of examples.\n",
        "4. Finally `prefetch` runs the dataset in parallel with the model to ensure that data is available when needed. See [Better performance with the `tf.data`](https://www.tensorflow.org/guide/data_performance.ipynb) for details."
      ],
      "metadata": {
        "id": "nO5Vv8eyDs1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "WtQFS44lH8vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  )"
      ],
      "metadata": {
        "id": "CgGVnlu7H__e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Dataset"
      ],
      "metadata": {
        "id": "bYaEzQIZDs3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create training and validation set batches\n",
        "train_batches = make_batched(train_examples)\n",
        "val_batches = make_batched(val_examples)"
      ],
      "metadata": {
        "id": "jcxfqyk4IYtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting `tf.data.Dataset` objects are setup for training with Keras.\n",
        "Keras `Model.fit` training expects `(inputs, labels)` pairs.\n",
        "The `inputs` are pairs of tokenized Portuguese and English sequences, `(pt, en)`.\n",
        "The `labels` are the same English sequences shifted by 1.\n",
        "This shift is so that at each location input `en` sequence, the `label` in the next token.\n"
      ],
      "metadata": {
        "id": "A2KoadZwDs5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>Inputs at the bottom, labels at the top.</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "H9ZvvQu1Ds8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same as the [text generation tutorial](text_generation.ipynb),\n",
        "except here you have additional input \"context\" (the Portuguese sequence) that the model is \"conditioned\" on.\n",
        "\n",
        "This setup is called \"teacher forcing\" because regardless of the model's output at each timestep, it gets the true value as input for the next timestep.\n",
        "This is a simple and efficient way to train a text generation model.\n",
        "It's efficient because you don't need to run the model sequentially, the outputs at the different sequence locations can be computed in parallel.\n",
        "\n",
        "You might have expected the `input, output`, pairs to simply be the `Portuguese, English` sequences.\n",
        "Given the Portuguese sequence, the model would try to generate the English sequence.\n",
        "\n",
        "It's possible to train a model that way. You'd need to write out the inference loop and pass the model's output back to the input.\n",
        "It's slower (time steps can't run in parallel), and a harder task to learn (the model can't get the end of a sentence right until it gets the beginning right),\n",
        "but it can give a more stable model because the model has to learn to correct its own errors during training."
      ],
      "metadata": {
        "id": "nhCco-lFDs-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (pt, en), en_labels in train_batches.take(1):\n",
        "    break\n",
        "\n",
        "print(pt.shape)\n",
        "print(en.shape)\n",
        "print(en_labels.shape)"
      ],
      "metadata": {
        "id": "mzll4QyJJZvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `en` and `en_labels` are the same, just shifted by 1:"
      ],
      "metadata": {
        "id": "A-x1mvZsDs_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(en[0][:10])\n",
        "print(en_labels[0][:10])"
      ],
      "metadata": {
        "id": "EOumr1qOJh3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the components"
      ],
      "metadata": {
        "id": "2oAarKDuDtB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a lot going on inside a Transformer. The important things to remember are:\n",
        "\n",
        "1. It follows the same general pattern as a standard sequence-to-sequence model with an encoder and a decoder.\n",
        "2. If you work through it step by step it will all make sense."
      ],
      "metadata": {
        "id": "XVD4ShbfDtDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The original Transformer diagram</th>\n",
        "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Each of the components in these two diagrams will be explained as you progress through the tutorial."
      ],
      "metadata": {
        "id": "-kNz9hYSJqu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The embedding and positional encoding layer"
      ],
      "metadata": {
        "id": "07k7D_U_Jqxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KAlJfrlzJqzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1gESc3QWJq13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_paNHldGJq4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RmOuRTWmJq6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cuqBEWYHJq81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ML20qEBvJq-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Df1CMJ8bJrBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gu7BHL9tJrDH"
      }
    }
  ]
}