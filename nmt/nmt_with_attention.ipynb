{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbIIpYZkOUKqN16GNTgBaW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/won-hj/deep_learning_study/blob/main/nmt/nmt_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ],
      "metadata": {
        "id": "Fp-IDEB5Xj4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E6u35AXK5P4"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural machine translation with attention"
      ],
      "metadata": {
        "id": "1-d0JADSXpPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 노트북을 기반으로 영어 번역 스페인어 순서 (seq2seq) 모델과 일련의 훈련 [주의 기반 신경 기계 번역에 접근 효과를](https://arxiv.org/abs/1508.04025v5) .\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN%2Battention-words-spa.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th colspan=1>This tutorial: An encoder/decoder connected by attention.</th>\n",
        "<tr>\n",
        "</table>\n",
        "이 아키텍처는 다소 오래된 동안 그것은 (에에 가기 전에주의 메커니즘의 깊은 이해를 얻을 수있는을 통해 작업에 매우 유용한 프로젝트 여전히 [트랜스포](https://www.tensorflow.org/text/tutorials/transformer?hl=ko&_gl=1*eeepfb*_up*MQ..*_ga*ODI4OTI5NTIyLjE3Mzk1MDE1NjM.*_ga_W0YLR4190T*MTczOTUwMTU2My4xLjAuMTczOTUwMTU2My4wLjAuMA..) ).\n",
        "\n",
        "\n",
        "이것은 다음 사항에 대한 지식이 있다고 가정하는 고급 예입니다.\n",
        "\n",
        "* Sequence to sequence 모델\n",
        "* keras 레이어 아래의 TensorFlow 기본 사항:\n",
        "  * [텐서로 직접 작업하기](https://www.google.com/url?q=https%3A%2F%2Fwww.tensorflow.org%2Fguide%2Ftensor)\n",
        "  * [사용자 정의 작성 `keras.Model` 들과 `keras.layers`](https://www.google.com/url?q=https%3A%2F%2Fwww.tensorflow.org%2Fguide%2Fkeras%2Fcustom_layers_and_models)\n",
        "\n",
        "\n",
        "\n",
        "\"? ¿ todavia estan 엉 카사\"이 노트북 모델을 훈련 한 후에는 같은 스페인의 문장을 입력 할 수 및 영어 번역 반환합니다 : \"집에서 아직도를\"\n",
        "\n",
        "생성 된 모델은 같은 내보낼 `tf.saved_model` 가 TensorFlow 다른 환경에서 사용될 수 있도록.\n",
        "\n",
        "번역 품질은 장난감 예에 적합하지만 생성된 주의 플롯이 더 흥미로울 수 있습니다. 이것은 번역하는 동안 입력 문장의 어느 부분이 모델의 주의를 끌었는지 보여줍니다.\n",
        "\n",
        "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
        "\n",
        "참고 :이 예제는 하나의 P100의 GPU에서 실행하기 위해 약 10 분 정도 소요됩니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iip_36PAYFp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 설정"
      ],
      "metadata": {
        "id": "Tozt23CQZy7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow-text>=2.11\"\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "RVbMQZ2VZ0bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ],
      "metadata": {
        "id": "cZtlcw7iZ2rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 튜토리얼은 처음부터 몇 개의 레이어를 빌드합니다. 사용자 정의 구현과 내장 구현 간에 전환하려면 이 변수를 사용하세요."
      ],
      "metadata": {
        "id": "jXWewOvtY4Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_builtins =True #?"
      ],
      "metadata": {
        "id": "ct2n6nSOaG63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 튜토리얼은 셰이프를 틀리기 쉬운 저수준 API를 많이 사용합니다. 이 클래스는 튜토리얼 전체에서 모양을 확인하는데 사용됩니다."
      ],
      "metadata": {
        "id": "X2TncOosaLPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class ShapeChecker():\n",
        "  def __init_(self):\n",
        "    #keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    '''\n",
        "    if isinstance(names, str):\n",
        "      names = (names, )\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.ranf(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'   found {rank}: {shape.numpy()}\\n'\n",
        "                       f'   expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "    if (broadcast and new_dim == 1):\n",
        "      continue\n",
        "\n",
        "    if old_dim is None:\n",
        "      #if the axis name is new, add its elngth to the cache\n",
        "      self.shapes[name] = new_dim\n",
        "      continue\n",
        "\n",
        "    if new_dim != old_dim:\n",
        "      raise ValueError(f'Shape mismatch for dimension: \"{name}\"\\n'\n",
        "      f'   found: {new_dim}\\n'\n",
        "      f'   expected: {old_dim}\\n')\n",
        "    '''\n",
        "    parsed = einops.parse_shape(tensor, name)\n",
        "\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        #if the axis name is new, add its elngth to the cache\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f'Shape mismatch for dimension: \"{name}\"\\n'\n",
        "        f'   found: {new_dim}\\n'\n",
        "        f'   expected: {old_dim}\\n')\n",
        ""
      ],
      "metadata": {
        "id": "phiTSlkjaVux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터"
      ],
      "metadata": {
        "id": "JbG1Fi2Pb_ZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리가 제공하는 언어 데이터 세트 사용합니다 http://www.manythings.org/anki/ 이 데이터 세트는 형식 언어 번역 쌍을 포함를:\n",
        "\n",
        "```\n",
        "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
        "```\n",
        "\n",
        "그들은 다양한 언어를 사용할 수 있지만 우리는 영어-스페인어 데이터 세트를 사용할 것입니다."
      ],
      "metadata": {
        "id": "bIIhg-ZccDNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터세트 다운로드 및 준비"
      ],
      "metadata": {
        "id": "cLRYSORocZNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "편의를 위해 Google Cloud에서 이 데이터세트의 사본을 호스팅했지만 자체 사본을 다운로드할 수도 있습니다. 데이터 세트를 다운로드한 후 데이터를 준비하기 위해 수행할 단계는 다음과 같습니다.\n",
        "1. 각 문장에 토큰 시작과 끝을 추가합니다.\n",
        "2. 특수 문자를 제거하여 문장을 정리합니다.\n",
        "3. 단어 색인 및 역단어 색인을 작성하십시오(단어->id 및 id->단어에서 사전 매핑).\n",
        "4, 각 문장을 최대 길이로 채웁니다."
      ],
      "metadata": {
        "id": "f1bTA534cbUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the file\n",
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True\n",
        ")\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
      ],
      "metadata": {
        "id": "BCIYzFmdbYq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def laod_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  '''inp = [inp for targ, inp in pairs]\n",
        "  targ = [targ for targ, inp in pairs]\n",
        "\n",
        "  return targ, inp'''\n",
        "  context = np.array([context for target, context in pairs])\n",
        "  target = np.array([target for target, context in pairs])\n",
        "\n",
        "  return target, context"
      ],
      "metadata": {
        "id": "zG7qRlXWdDJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#targ, inp = load_data(path_to_file)\n",
        "target_raw, context_raw = load_data(path_to_file)\n",
        "print(context_raw[-1] if context_raw else inp[-1])\n",
        "print()\n",
        "print(target_raw[-1] if target_raw else targ[-1])"
      ],
      "metadata": {
        "id": "0VGdv_SpdWMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tf.data 데이터세트 만들기\n"
      ],
      "metadata": {
        "id": "773NjcTYaVAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "문자열이 배열에서 당신은 만들 수 있습니다 [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=ko&_gl=1*1rrf9ph*_up*MQ..*_ga*ODI4OTI5NTIyLjE3Mzk1MDE1NjM.*_ga_W0YLR4190T*MTczOTUwMTU2My4xLjAuMTczOTUwMTU2My4wLjAuMA..) 그 섞어 배치 그들을 효율적으로 문자열을 :"
      ],
      "metadata": {
        "id": "MND_qdx9dsOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "#dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "train_raw  = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[istrain], target_raw[is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train] target_raw[~is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "sB62m7vbdxfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "for ex_input_batch, ex_target_batch in dataset.take(1):\n",
        "  print(ex_input_batch[:5])\n",
        "  print()\n",
        "  print(ex_target_batch[:5])\n",
        "  break"
      ],
      "metadata": {
        "id": "s-EN7b2Td-Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example_context_strings, example_target_strings in train_raw.take(1):\n",
        "  print(example_context_strings[:5])\n",
        "  print()\n",
        "  print(example_target_strings[:5])\n",
        "  break"
      ],
      "metadata": {
        "id": "-V2d-rDo1Uec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트 전처리"
      ],
      "metadata": {
        "id": "rboSLFIceLPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 튜토리얼의 목표 중 하나는로 내보낼 수 있습니다 모델 구축하는 것입니다 [`tf.saved_model`](https://www.tensorflow.org/api_docs/python/tf/saved_model?hl=ko&_gl=1*1l83uhq*_up*MQ..*_ga*ODI4OTI5NTIyLjE3Mzk1MDE1NjM.*_ga_W0YLR4190T*MTczOTUwMTU2My4xLjAuMTczOTUwMTU2My4wLjAuMA..) . 그 수출 모델을 편리하게 사용할 수 있도록하기 위해이해야 [`tf.string`](https://www.tensorflow.org/api_docs/python/tf?hl=ko&_gl=1*1nqs6rn*_up*MQ..*_ga*ODI4OTI5NTIyLjE3Mzk1MDE1NjM.*_ga_W0YLR4190T*MTczOTUwMTU2My4xLjAuMTczOTUwMTU2My4wLjAuMA..#string) 입력을 반환 [`tf.string`](https://www.tensorflow.org/api_docs/python/tf?hl=ko&_gl=1*1nqs6rn*_up*MQ..*_ga*ODI4OTI5NTIyLjE3Mzk1MDE1NjM.*_ga_W0YLR4190T*MTczOTUwMTU2My4xLjAuMTczOTUwMTU2My4wLjAuMA..#string) 출력 : 모든 텍스트 처리는 모델 내부에서 발생합니다."
      ],
      "metadata": {
        "id": "_r5gPNyweMhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 표준화"
      ],
      "metadata": {
        "id": "mpQKe5_Ou4Mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 모델은 어휘가 제한된 다국어 텍스트를 다루고 있습니다. 따라서 입력 텍스트를 표준화하는 것이 중요합니다.\n",
        "\n",
        "첫 번째 단계는 악센트가 있는 문자를 분할하고 호환성 문자를 해당 ASCII 문자로 바꾸는 유니코드 정규화입니다.\n",
        "\n",
        "`tensorflow_text` 패키지는 유니 코드 정규화 작업을 포함 :"
      ],
      "metadata": {
        "id": "4iyc-Nwwu8E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = tf.constant('¿Todavía está en casa?')\n",
        "\n",
        "print(example_text.numpy())\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ],
      "metadata": {
        "id": "K6IFpKbqvA4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "유니코드 정규화는 텍스트 표준화 기능의 첫 번째 단계가 될 것입니다."
      ],
      "metadata": {
        "id": "y8865h5ZvNcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  #split accecented characters\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  #keep space, a-z and select punctuation\n",
        "  text = tf.strings.regex_replace(text, '[& a-z.?!,¿]', '')\n",
        "  #add spaces around punctuation\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  #strip whitespace\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "9vJ-DaTnvRve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_text.numpy().decode())\n",
        "print(tf.lower_and_split_punct(example_text).numpy().decode())"
      ],
      "metadata": {
        "id": "pTRjltDSwFHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 텍스트 벡터화"
      ],
      "metadata": {
        "id": "C5iJcvtYwYUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 표준화 함수는 싸서한다 [`tf.keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization?hl=ko&_gl=1*168ghyd*_up*MQ..*_ga*ODI4OTI5NTIyLjE3Mzk1MDE1NjM.*_ga_W0YLR4190T*MTczOTUwMTU2My4xLjAuMTczOTUwMTU2My4wLjAuMA..) 토큰 서열 어휘 추출 및 입력 텍스트의 변환을 처리 층."
      ],
      "metadata": {
        "id": "3HdsC6wdwbwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_token=max_vocab_size,\n",
        "    ragged=True\n",
        ")"
      ],
      "metadata": {
        "id": "1fdw6xBFwkNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`TextVectorization` 층 및 기타 전처리 층은 [Kers preprocessing layers](https://www.google.com/url?q=https%3A%2F%2Fwww.tensorflow.org%2Fguide%2Fkeras%2Fpreprocessing_layers)가지고 `adapt` 방법. 이 방법은 훈련 데이터의 한 시대를 읽고, 같은 많은 작업 `Model.fix` . 이 `adapt` 방법은 데이터에 기초 층을 초기화한다. 여기에서 어휘를 결정합니다."
      ],
      "metadata": {
        "id": "3TkcWBmowv34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_text_processor.adpat(inp)\n",
        "context_text_precessor.adapt(train_raw.map(lambda target, context: context))\n",
        "\n",
        "#here are the first 10 words from the vocabulary\n",
        "#input_text_processor.get_vocabualary()[:10]\n",
        "context_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "OOZKOZFnw2ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "즉 스페인의 `TextVectorization` 구축하고 현재 레이어 `.adapt()` 영어 하나"
      ],
      "metadata": {
        "id": "3ekg98yHxHzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#output_text_processor = tf.keras.layers.TextVectorization(\n",
        "target_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)\n",
        "\n",
        "#output_text_processor.adapt(targ)\n",
        "#output_text_processor.get_vocabulary()[:10]\n",
        "target_text_processor.adapt(train_raw.map(lambda target, context: target))\n",
        "target_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "GrN-4h2uxLII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 이러한 레이어는 문자열 배치를 토큰 ID 배치로 변환할 수 있습니다."
      ],
      "metadata": {
        "id": "FTJqkDCcxh8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#example_tokens = input_text_processor(example_input_batch)\n",
        "#example_tokens[:3, :10]\n",
        "example_tokens = context_text_processor(example_context_strings)\n",
        "example_tokens[:3, :10]"
      ],
      "metadata": {
        "id": "vUPTEw5jxjP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_vocabulary` 방법은 텍스트로 다시 토큰 ID를 변환 할 수 있습니다 :"
      ],
      "metadata": {
        "id": "SH_cShEUxwgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "#tokens = input_vocab[example_tokens[0].numpy()]\n",
        "#''.join(tokens)\n",
        "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
        "tokens = context_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ],
      "metadata": {
        "id": "kSgdkXBTxyVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "반환된 토큰 ID는 0으로 채워집니다. 이것은 쉽게 마스크로 바뀔 수 있습니다."
      ],
      "metadata": {
        "id": "OuJ1JVe7x-pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.pcolormesh(example_tokens.to_tensor())\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "ple.pcolormesh(exxample_tokens.to_tensor() != 0)\n",
        "plt.title('Mask')"
      ],
      "metadata": {
        "id": "rn-bkAm0yBr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the dataset"
      ],
      "metadata": {
        "id": "sDMl6bq8yaw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `process_text` function below converts the `Datasets` of strings, into  0-padded tensors of token IDs. It also converts from a `(context, target)` pair to an `((context, target_in), target_out)` pair for training with `keras.Model.fit`. Keras expects `(inputs, labels)` pairs, the inputs are the `(context, target_in)` and the labels are `target_out`. The difference between `target_in` and `target_out` is that they are shifted by one step relative to eachother, so that at each location the label is the next token."
      ],
      "metadata": {
        "id": "YZt6lqBJyfEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(context, target):\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  target = target_text_processor(target)\n",
        "  targ_in = target[:.:-1].to_tensor()\n",
        "  targ_out = target[:, 1:].to_tensor()\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "UIfn-zfoyf7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the first sequence of each, from the first batch:"
      ],
      "metadata": {
        "id": "BjC4SUCf8FkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
        "  print(ex_context_tok[0, :10].numpy())\n",
        "  print()\n",
        "  print(ex_tar_in[0, :10].numpy())\n",
        "  print(ex_tar_out[0, :10].numpy())"
      ],
      "metadata": {
        "id": "0C18pTXa8Hw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 인코더/디코더  \n"
      ],
      "metadata": {
        "id": "js4zz6qe8Q1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 다이어그램의 파란색 부분인 인코더를 빌드하는 것으로 시작합니다.\n",
        "\n",
        "인코더:\n",
        "\n",
        "1. (에서 토큰 ID 목록을 취 input_text_processor ).\n",
        "2. 각 토큰에 대한 내장 벡터 조회 (A 사용 layers.Embedding ).\n",
        "3. 새로운 시퀀스에 묻어 (A 사용하여 처리 layers.GRU ).\n",
        "4. 보고:\n",
        "  * 처리된 시퀀스입니다. 이것은 주의 헤드로 전달됩니다.\n",
        "  * 내부 상태입니다. 디코더를 초기화하는 데 사용됩니다.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=380 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th colspan=1>The original from <a href=https://arxiv.org/abs/1508.04025v5>Effective Approaches to Attention-based Neural Machine Translation</a></th>\n",
        "  <th colspan=1>This tutorial's model</th>\n",
        "<tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "cyumSAz48XmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before getting into it define constants for the model:"
      ],
      "metadata": {
        "id": "LNBGN-DQ9KZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNITS = 256"
      ],
      "metadata": {
        "id": "Wq4rjzRA9LBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The encoder\n",
        "\n",
        "The goal of the encoder is to process the context sequence into a sequence of vectors that are useful for the decoder as it attempts to predict the next output for each timestep. Since the context sequence is constant, there is no restriction on how information can flow in the encoder, so use a bidirectional-RNN to do the processing:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://tensorflow.org/images/tutorials/transformer/RNN-bidirectional.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>A bidirectional RNN</th>\n",
        "<tr>\n",
        "</table>\n",
        "\n",
        "The encoder:\n",
        "\n",
        "1. Takes a list of token IDs (from `context_text_processor`).\n",
        "3. Looks up an embedding vector for each token (Using a `layers.Embedding`).\n",
        "4. Processes the embeddings into a new sequence (Using a bidirectional `layers.GRU`).\n",
        "5. Returns the processed sequence. This will be passed to the attention head."
      ],
      "metadata": {
        "id": "kJ8dPlbw9OPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    #the embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    #the GRU RNN layer processses those vectors sequentially\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   #return the sequence and sate\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    #2. the embedding layer looks up the embedding for each token\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vector, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    #3 the gru processes the embedding sequence\n",
        "    #  output shape: (batch, s, enc_units)\n",
        "    #  state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    #4 returns the new sequence and its state\n",
        "    return output, state\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "\n",
        "    # the embedding layer converts tokens to vector\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True)\n",
        "\n",
        "    #the rnn layer processes those vectors sequentially\n",
        "    self.rnn = tf.keras.layers.Bidirectional(\n",
        "        merge_mode = 'sum',\n",
        "        layer = tf.keras.layers.GRU(units,\n",
        "                                    #return the sequence and state\n",
        "                                    return_sequence=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch s')\n",
        "\n",
        "    #2 the embedding layer look up the embedding vector for each token\n",
        "    s = self.embedding(x)\n",
        "    shape_checker(x, 'batch s uits')\n",
        "\n",
        "    # the gru processes the sequence of embeddings\n",
        "    x = self.rnn(x)\n",
        "    shape_cehcker(x, 'batch s units')\n",
        "\n",
        "    #4 return the new sequence of embedding\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context"
      ],
      "metadata": {
        "id": "54Oi9Hiv81n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이것이 지금까지 어떻게 일치하는지입니다."
      ],
      "metadata": {
        "id": "tpSPOo7NAK1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "#convert the input text to tokens\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "#encode the input sequence\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')\n",
        "\n",
        "\n",
        "#Encoder the input sequence\n",
        "encoder = Encoder(input_text_processor, UNITS)\n",
        "ex_context = encoder(ex_context_tok)\n",
        "\n",
        "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
      ],
      "metadata": {
        "id": "Rdv1QLPNAM17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더는 해당 상태를 디코더 초기화에 사용할 수 있도록 내부 상태를 반환합니다.\n",
        "\n",
        "RNN이 여러 호출을 통해 시퀀스를 처리할 수 있도록 상태를 반환하는 것도 일반적입니다. 디코더를 만드는 과정을 더 많이 보게 될 것입니다."
      ],
      "metadata": {
        "id": "F-A-E1yBAg3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The attention layer\n",
        "\n",
        "The attention layer lets the decoder access the information extracted by the encoder. It computes a vector from the entire context sequence, and adds that to the decoder's output.\n",
        "\n",
        "The simplest way you could calculate a single vector from the entire sequence would be to take the average across the sequence (`layers.GlobalAveragePooling1D`). An attention layer is similar, but calculates a **weighted** average across the context sequence. Where the weights are calculated from the combination of context and \"query\" vectors.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th colspan=1>The attention layer</th>\n",
        "<tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "디코더는 주의를 사용하여 입력 시퀀스의 일부에 선택적으로 초점을 맞춥니다. Attention은 각 예제에 대한 입력으로 벡터 시퀀스를 사용하고 각 예제에 대해 \"attention\" 벡터를 반환합니다. 이 관심 층은 비슷 `layers.GlobalAveragePoling1D` 하지만 관심 층은 **가중 평균**을 행한다.\n",
        "\n",
        "작동 방식을 살펴보겠습니다.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th colspan=1>The attention layer</th>\n",
        "<tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "v5_jtqylCWST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_head=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    shape_checker = ShapeChecker()\n",
        "\n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(context, 'batch s units')\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=context,\n",
        "        return_attention_scores=True\n",
        "    )\n",
        "\n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(attn_scores, 'batch heads t s')\n",
        "\n",
        "    # cache the attention scores for plotting layer\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    shape_checker(attn_scores, 'batch t s')\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "QT3H_61XDFLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attnetion_layer = CrossAttention(UNITS)\n",
        "\n",
        "# attend to the enceded tokens\n",
        "embed = tf.keras.ayers.Embedding(target_text_processor.vocabyllary_size(),\n",
        "                                 output_dim_UNITS, mask_zero=True)\n",
        "ex_tar_embed = embed(ex_tar_in)\n",
        "\n",
        "result = attention_layer(ex_tar_embed, ex_context)\n",
        "\n",
        "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
        "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
        "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
        "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
      ],
      "metadata": {
        "id": "owg8yfxuD-Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention weights will sum to `1` over the context sequence, at each location in the target sequence."
      ],
      "metadata": {
        "id": "sjD3cH2lCn40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
      ],
      "metadata": {
        "id": "1IaP5Rz-EUs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Here are the attention weights across the context sequences at `t=0`:"
      ],
      "metadata": {
        "id": "BhwdK9J-Ebt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights = attention_layer.last_attention_weights\n",
        "mask(ex_context_tok != 0).numpy()\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(mask)\n",
        "plt.title('Mask');\n"
      ],
      "metadata": {
        "id": "Q0UaJSDlEfP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because of the small-random initialization the attention weights are initially all close to `1/(sequence_length)`. The model will learn to make these less uniform as training progresses."
      ],
      "metadata": {
        "id": "tFNVTyceEosk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### attention head"
      ],
      "metadata": {
        "id": "1kajn__uEzik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더는 주의를 사용하여 입력 시퀀스의 일부에 선택적으로 초점을 맞춥니다. Attention은 각 예제에 대한 입력으로 벡터 시퀀스를 사용하고 각 예제에 대해 \"attention\" 벡터를 반환합니다. 이 관심 층은 비슷 `layers.GlobalAveragePoling1D` 하지만 관심 층은 가중 평균을 행한다.\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/text/tutorials/images/attention_equation_1.jpg?hl=ko\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>A unidirectional RNN</th>\n",
        "<tr>\n",
        "</table>\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"data:image/png;base64,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\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "CVlx5FLgE32B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막은이다 ***score***\n",
        " 기능. 그 작업은 각 키-쿼리 쌍에 대한 스칼라 로짓 점수를 계산하는 것입니다. 두 가지 일반적인 접근 방식이 있습니다.\n",
        "\n",
        "\n",
        " <table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/text/tutorials/images/attention_equation_4.jpg?hl=ko\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>A unidirectional RNN</th>\n",
        "<tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "CzwGQKjEFZPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 튜토리얼은 사용 [Bahdanau의 첨가제 관심](https://arxiv.org/pdf/1409.0473.pdf)을 . TensorFlow은 모두 구현 포함 [`layers.Attention`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention?hl=ko&_gl=1*1h4lx4j*_up*MQ..*_ga*ODI4OTI5NTIyLjE3Mzk1MDE1NjM.*_ga_W0YLR4190T*MTczOTUwMTU2My4xLjAuMTczOTUwMTU2My4wLjAuMA..) 및 [`layers.AdditiveAttention`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention?hl=ko&_gl=1*1h4lx4j*_up*MQ..*_ga*ODI4OTI5NTIyLjE3Mzk1MDE1NjM.*_ga_W0YLR4190T*MTczOTUwMTU2My4xLjAuMTczOTUwMTU2My4wLjAuMA..) . 핸들 아래 클래스의 쌍 가중치 행렬 [`layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?hl=ko&_gl=1*1h4lx4j*_up*MQ..*_ga*ODI4OTI5NTIyLjE3Mzk1MDE1NjM.*_ga_W0YLR4190T*MTczOTUwMTU2My4xLjAuMTczOTUwMTU2My4wLjAuMA..) 층, 및 내장 구현을 요구한다."
      ],
      "metadata": {
        "id": "4_wHUde6Fk3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    #for eqn. (4), the bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = =tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker=ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    #from eqn4, 'W1@ht'\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    #from eqn4, 'W2@hs'\n",
        "    s2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores=True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "0hMUYKVcHa5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "어텐션 레이어 테스트"
      ],
      "metadata": {
        "id": "6S91D2hJHUSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "크리에이트 `BahdanauAttention` 레이어를 :"
      ],
      "metadata": {
        "id": "yk36QBELIyjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = BahdanauAttention(units)"
      ],
      "metadata": {
        "id": "HgW8psV6I1zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 레이어는 3개의 입력을 받습니다:\n",
        "\n",
        "* query :이 나중에, 디코더에 의해 생성됩니다.\n",
        "* value 이 인코더의 출력됩니다.\n",
        "* mask : 패딩을 제외하려면 example_tokens != 0"
      ],
      "metadata": {
        "id": "NQ6CwVfTI6Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(example_tokens != 0).shape"
      ],
      "metadata": {
        "id": "icHIK_XJI8ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "어텐션 레이어의 벡터화된 구현을 통해 쿼리 벡터 시퀀스의 배치와 값 벡터 시퀀스의 배치를 전달할 수 있습니다. 결과는 다음과 같습니다.\n",
        "\n",
        "1. 쿼리 크기의 결과 벡터 시퀀스 배치입니다.\n",
        "2. 일괄주의는 크기, 매핑 `(query_length, value_length)` ."
      ],
      "metadata": {
        "id": "3H3eMYpVI_m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# latter, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.noraml(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "#attend to the enceded tokens\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_token != 0)\n",
        ")\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ],
      "metadata": {
        "id": "PW8A1tiGJE_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "관심 가중치 합계는해야 `1.0` 각 시퀀스.\n",
        "\n",
        "여기에서 시퀀스에서 관심의 무게입니다 `t=0`:"
      ],
      "metadata": {
        "id": "SJbmS0VCJaIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ],
      "metadata": {
        "id": "ozoyC3fWJdks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "때문에 작은 임의 초기의 관심 가중치는 모든 가까운 `1/(sequence_length)` . 단일 순서의 무게에 확대하면 모델이 확장 배우고, 활용할 수있는 몇 가지 작은 변화가 있음을 알 수있다."
      ],
      "metadata": {
        "id": "0b0p0VC_Jfea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " attention_weights.shape"
      ],
      "metadata": {
        "id": "pjwCvRtnJhf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_slice = attention_seights[0,0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]"
      ],
      "metadata": {
        "id": "OgJGAYI4Ji7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
      ],
      "metadata": {
        "id": "isx-rIqxJqJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 디코더"
      ],
      "metadata": {
        "id": "FQvi1ztIErwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더의 작업은 다음 출력 토큰에 대한 예측을 생성하는 것입니다.\n",
        "1. 디코더는 완전한 인코더 출력을 수신합니다.\n",
        "2. RNN을 사용하여 지금까지 생성한 내용을 추적합니다.\n",
        "3. RNN 출력을 인코더의 출력에 대한 주의에 대한 쿼리로 사용하여 컨텍스트 벡터를 생성합니다.\n",
        "4. 그것은 \"주의 벡터\"를 생성하기 위해 식 3(아래)을 사용하여 RNN 출력과 컨텍스트 벡터를 결합합니다.\n",
        "5. \"주의 벡터\"를 기반으로 다음 토큰에 대한 로짓 예측을 생성합니다.\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "<img width=500 src='https://www.tensorflow.org/text/tutorials/images/attention_equation_3.jpg?hl=ko' />\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "cAmwXdQaJwLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기입니다 `Decoder` 클래스와 그 초기화. 이니셜라이저는 필요한 모든 레이어를 생성합니다."
      ],
      "metadata": {
        "id": "lRshFZVbKFJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethos\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return run\n",
        "\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    seslf.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token'', oov_token='[UNK]' #oov?\n",
        "    )\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(\n",
        "        vocabular=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]',\n",
        "        invert=True\n",
        "    )\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "    #1 the embedding layer converts token ids to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
        "                                               units, mask_zero=True)\n",
        "\n",
        "    #2 the rnn keeps track of what's been generated so far\n",
        "    self.rnn = tf.keras.layers.GRU(unis,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True, #dho??\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    #3 the rnn output will be the query for the attention layer\n",
        "    self.attention = CrossAttention(units)\n",
        "\n",
        "    #4 this fully connected layer produces the logits for each\n",
        "    # output token\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
      ],
      "metadata": {
        "id": "6kFpdFDCKIJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 훈련"
      ],
      "metadata": {
        "id": "inkDKa7LUyoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the `call` method, takes 3 arguments:\n",
        "\n",
        "* `inputs` -  a `context, x` pair where:\n",
        "  * `context` - is the context from the encoder's output.\n",
        "  * `x` - is the target sequence input.\n",
        "* `state` - Optional, the previous `state` output from the decoder (the internal state of the decoder's RNN). Pass the state from a previous run to continue generating text where you left off.\n",
        "* `return_state` - [Default: False] - Set this to `True` to return the RNN state."
      ],
      "metadata": {
        "id": "GQP7JSyYYHRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Decoder.add_method #저런식으로 사용한다는 이유가 뭘까? 클래스 안에 구현하지 않는 이유?\n",
        "def call(self, context, x, state=None, return_state=False):\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_cheker(x, 'batch t')\n",
        "  shape_checker(context, 'batch s units')\n",
        "\n",
        "  #1 lookup the embeddings\n",
        "  x = self.embedding(x)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  #2 process the target sequence\n",
        "  x, state = self.rnn(x, initial_state=state)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  #3 use the rnn output as the query for the attention over the context\n",
        "  x = self.attention(x, context)\n",
        "  self.last_attention_weights = self.attention.last_attention_weights\n",
        "  shape_checker(x, 'batch t units')\n",
        "  shape_checker(self.last_attention_weights, 'batch t s')\n",
        "\n",
        "  #4 generate logit predictions for the next token\n",
        "  logits = self.output_layer(x)\n",
        "  shape_checker(logits, 'batch t target_vocab_size')\n",
        "\n",
        "  if return_state:\n",
        "    return logits, state\n",
        "  else:\n",
        "    return logits"
      ],
      "metadata": {
        "id": "W04NCt_AYYEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That will be sufficient for training. Create an instance of the decoder to test out:"
      ],
      "metadata": {
        "id": "hLqAammHaEil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(target_text_processor, UNITS)"
      ],
      "metadata": {
        "id": "mrsk73qxaGjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In training you'll use the decoder like this:\n",
        "\n",
        "Given the context and target tokens, for each target token it predicts the next target token."
      ],
      "metadata": {
        "id": "eqWCL4f7aKON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = decoder(ex_context, ex_tar_in)\n",
        "\n",
        "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
        "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
      ],
      "metadata": {
        "id": "TQ6czxJMaMmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference\n",
        "\n",
        "To use it for inference you'll need a couple more methods:"
      ],
      "metadata": {
        "id": "9nCLabtXaK66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
      ],
      "metadata": {
        "id": "Y3kCJ1tgaach"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  words = self.id_to_word(tokens)\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *&', '')\n",
        "  return result"
      ],
      "metadata": {
        "id": "8Fvh0XTgawGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "  logits, state = self(context, next_token, state = state, return_state=True)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  #if a sequence produces an 'end_token', set it 'done\n",
        "  done = done | (next_token == self.end_token)\n",
        "  #once a sequence is donee it only produces 0-padding\n",
        "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "  return next_token, done, state"
      ],
      "metadata": {
        "id": "RS-Ewck6bRLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With those extra functions, you can write a generation loop:"
      ],
      "metadata": {
        "id": "uhXcMYLGaZz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setup the loop variables\n",
        "next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "tokens=[]\n",
        "\n",
        "for n in range(10):\n",
        "  #run one step\n",
        "  next_token, done, state = decoder.get_nexxt_token(\n",
        "      ex_context, next_token, done, state, temperature=1.0\n",
        "  )\n",
        "  #add the token to the output\n",
        "  tokens.append(next_token)\n",
        "\n",
        "#stack all the tokens together\n",
        "tokens = tf.concat(tokens, axis=-1) #(batch, t)\n",
        "\n",
        "#convert the tokens back to a a string ->> \"a\" string? or 'a \"a\" string'?\n",
        "result = decoder.tokens_to_text(tokens)\n",
        "result[:3].numpy()"
      ],
      "metadata": {
        "id": "-iUNHvtnb97Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model's untrained, it outputs items from the vocabulary almost uniformly at random."
      ],
      "metadata": {
        "id": "Y5IQSHAKaTcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model\n",
        "\n",
        "Now that you have all the model components, combine them to build the model for training:"
      ],
      "metadata": {
        "id": "F_V1_fN8crkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fnn\n",
        "\n",
        "  def __init__(self, units,\n",
        "               context_text_processor,\n",
        "               target_text_processor):\n",
        "    super().__init__()\n",
        "    #build the encoder and decoder\n",
        "    encoder = Encoder(context_text_processor, units)\n",
        "    decoder = Decoder(context_text_processor, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x = inputs\n",
        "    context = self.encoder(context)\n",
        "    logits = self.decoder(context, x)\n",
        "\n",
        "    #todo(b/250038731) :remove this  ## ()안에 무슨 뜻???\n",
        "    try:\n",
        "      #delete the keras mask, so keras doesn't sccale the loss+accuracy\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "0dT_RWf9c0Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training the model will be used like this:"
      ],
      "metadata": {
        "id": "tfXSyhqrczlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
        "\n",
        "logits = model((ex_context_tok, ex_tar_in))\n",
        "\n",
        "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
        "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
      ],
      "metadata": {
        "id": "yAKaUrtjdvLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "xq61ilQeczob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training, you'll want to implement your own masked loss and accuracy functions:"
      ],
      "metadata": {
        "id": "nPlpDyaEd7cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "  #calculate the loss for each item in the batch\n",
        "  loss_fn = tf.keras.lossses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none'\n",
        "  )\n",
        "  loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "  #mask off the losses on padding\n",
        "  mask = tf.cast(y_true != 0, loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  #return the total\n",
        "  return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "Tu1QK-46d87w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "  #calculate the loss for each item in the batch\n",
        "  y_pred = tf.argmax(y_pred, axis=-1)\n",
        "  y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "  match = tf.cast(y_true == y_pred, tf.float32)\n",
        "  mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "  return tf.reuce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "nI01vnJUeYms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the model for training:"
      ],
      "metadata": {
        "id": "rm0knTujczsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nodel.compile(optimizer='adam',\n",
        "              loss=masked_loss,\n",
        "              metrics=[masked_acc, masked_loss])"
      ],
      "metadata": {
        "id": "Ni8mrsWle0dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is randomly initialized, and should give roughly uniform output probabilities. So it's easy to predict what the initial values of the metrics should be:"
      ],
      "metadata": {
        "id": "XFeZ5Mz8czvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
        "\n",
        "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
        " \"expected_acc\": 1/vocab_size}"
      ],
      "metadata": {
        "id": "dYxQhnNGe8fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That should roughly match the values returned by running a few steps of evaluation:"
      ],
      "metadata": {
        "id": "Z8Gdglo7cxVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_ds, steps=20, return_dict=True)"
      ],
      "metadata": {
        "id": "mornjbvefJaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_ds.repeat(),\n",
        "                    epochs=100,\n",
        "                    steps_per_epoch = 100,\n",
        "                    validation_data = val_ds,\n",
        "                    validation_steps=20,\n",
        "                    callbacks=[\n",
        "                        tf.keras.callbacks.EarlyStopping(patient=3)\n",
        "                    ])"
      ],
      "metadata": {
        "id": "g1XPOVGqfNg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()\n",
        "\n",
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "xYdAvZ25fdiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translate\n",
        "\n",
        "Now that the model is trained, implement a function to execute the full `text => text` translation. This code is basically identical to the [inference example](#inference) in the [decoder section](#the_decoder), but this also captures the attention weights."
      ],
      "metadata": {
        "id": "6qyR2i3AfJ2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def translate(self, texts, *, max_length=50, temperature=0.0):\n",
        "  #process the input texts\n",
        "  context = self.encoder.convert_input(texts)\n",
        "  batch_size = tf.shape(texts)[0]\n",
        "\n",
        "  #setup the loop inputs\n",
        "  tokens = []\n",
        "  attention_weights = []\n",
        "  next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    #generate the next token\n",
        "    next_token, done, state = self.decoder.get_next_token(\n",
        "        context, next_token, done, state, temperature\n",
        "    )\n",
        "    #collect the generated tokens\n",
        "    tokens.append(next_token)\n",
        "    attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  #stack the lists of tokens and attention weights\n",
        "  tokens = tf.concat(tokens, axis=-1) #t*[(batch 1)] -> (batch, t)\n",
        "  self.last_attention_weights = tf.concat(attention_weights, axis=1) #t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "  resule = self.decoder.tokens_to_text(tokens)\n",
        "  return result"
      ],
      "metadata": {
        "id": "uRxUZihnfjyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the two helper methods, used above, to convert tokens to text, and to get the next token:"
      ],
      "metadata": {
        "id": "69LEbakMfJ5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.translate(['¿Todavía está en casa?'])\n",
        "result[0].numpy().decode()"
      ],
      "metadata": {
        "id": "r2ehzZRCgxUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use that to generate the attention plot:"
      ],
      "metadata": {
        "id": "Q5r3CaG7fJ93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def plot_attention(self, text, **kwargs):\n",
        "  assert isinstance(text, str)\n",
        "  output = self.translate([text], **kwargs)\n",
        "  output = output[0].numpy().decode()\n",
        "\n",
        "  attention = self.last_attention_weights[0]\n",
        "\n",
        "  context = tf_lower_and_split_punct(text)\n",
        "  context = context.numpy().decode().split()\n",
        "\n",
        "  output = tf_lower_and_split_punct(output)\n",
        "  output = output.numpy().decode().split()[1:]\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')"
      ],
      "metadata": {
        "id": "O5nX5D7Eg8ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.plot_attention('¿Todavía está en casa?')"
      ],
      "metadata": {
        "id": "GBu9OGRdhgKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate a few more sentences and plot them:"
      ],
      "metadata": {
        "id": "phMeNXwjfKBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.plot_attention('Esta es mi vida')"
      ],
      "metadata": {
        "id": "_QztOBnohmVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.plot_attention('Tratar de descubrir.')"
      ],
      "metadata": {
        "id": "n6HRvJ5bhrRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The short sentences often work well, but if the input is too long the model literally loses focus and stops providing reasonable predictions. There are two main reasons for this:\n",
        "\n",
        "1. The model was trained with teacher-forcing feeding the correct token at each step, regardless of the model's predictions. The model could be made more robust if it were sometimes fed its own predictions.\n",
        "2. The model only has access to its previous output through the RNN state. If the RNN state looses track of where it was in the context sequence there's no way for the model to recover. [Transformers](transformer.ipynb) improve on this by letting the decoder look at what it has output so far."
      ],
      "metadata": {
        "id": "3xJdF3pthlP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `translate` function works on batches, so if you have multiple texts to translate you can pass them all at once, which is much more efficient than translating them one at a time:"
      ],
      "metadata": {
        "id": "pDpXoCvjhlT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [\n",
        "    'Hace mucho frio aqui.', # \"It's really cold here.\"\n",
        "    'Esta es mi vida.', # \"This is my life.\"\n",
        "    'Su cuarto es un desastre.' # \"His room is a mess\"\n",
        "]"
      ],
      "metadata": {
        "id": "HfErvi-ZiHZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for t in inputs:\n",
        "  print(model.translate([t])[0].numpy().decode())\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "B8EfTveoiPy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = model.translate(inputs)\n",
        "\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "nrPlbwtbiZ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So overall this text generation function mostly gets the job done, but so you've only used it here in python with eager execution. Let's try to export it next:"
      ],
      "metadata": {
        "id": "Zj8eAG_VhlYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export"
      ],
      "metadata": {
        "id": "uelmzH6vhlcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to export this model you'll need to wrap the `translate` method in a `tf.function`. That implementation will get the job done:\n"
      ],
      "metadata": {
        "id": "aux9GZElhlfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Export(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "  def translate(self, inputs):\n",
        "    return self.model.translate(inputs)"
      ],
      "metadata": {
        "id": "gKQlSjVDilBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export = Export(model)"
      ],
      "metadata": {
        "id": "FpTPIgQ_i0_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the `tf.function` once to compile it:"
      ],
      "metadata": {
        "id": "PnnRQYq7hlis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "_ = export.translate(tf.constant(inputs))"
      ],
      "metadata": {
        "id": "fTQqr-h-i4u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = export.translate(tf.constant(inputs))\n",
        "\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "pb5GAfHEi-Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the function has been traced it can be exported using `saved_model.save`:"
      ],
      "metadata": {
        "id": "gNUuv0jshll_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tf.saved_model.save(export, 'translator',\n",
        "                    signatures={'serving_default':export.translate})"
      ],
      "metadata": {
        "id": "mgnuqw29jFXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "reloaded = tf.saved_model.load('translator')\n",
        "_ = reloaded.translate(tf.constant(inputs)) #warmup"
      ],
      "metadata": {
        "id": "4l3JAfqSjOH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = reloaded.translate(tf.constant(inputs))\n",
        "\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "zGGjczycjVc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Optional] Use a dynamic loop"
      ],
      "metadata": {
        "id": "GPR4RjhSjbk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's worth noting that this initial implementation is not optimal. It uses a python loop:\n",
        "\n",
        "```\n",
        "for _ in range(max_length):\n",
        "  ...\n",
        "  if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "    break\n",
        "```\n",
        "\n",
        "The python loop is relatively simple but when `tf.function` converts this to a graph, it **statically unrolls** that loop. Unrolling the loop has two disadvantages:\n",
        "\n",
        "1. It makes `max_length` copies of the loop body. So the generated graphs take longer to build, save and load.\n",
        "1. You have to choose a fixed value for the `max_length`.\n",
        "1. You can't `break` from a statically unrolled loop. The `tf.function`\n",
        "  version will run the full `max_length` iterations on every call.\n",
        "  That's why the `break` only works with eager execution. This is\n",
        "  still marginally faster than eager execution, but not as fast as it could be.\n"
      ],
      "metadata": {
        "id": "zQ6iwk60jboY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix these shortcomings, the `translate_dynamic` method, below, uses a tensorflow loop:\n",
        "\n",
        "```\n",
        "for t in tf.range(max_length):\n",
        "  ...\n",
        "  if tf.reduce_all(done):\n",
        "      break\n",
        "```\n",
        "\n",
        "It looks like a python loop, but when you use a tensor as the input to a `for` loop (or the condition of a `while` loop) `tf.function` converts it to a dynamic loop using operations like `tf.while_loop`.\n",
        "\n",
        "There's no need for a `max_length` here it's just in case the model gets stuck generating a loop like: `the united states of the united states of the united states...`.\n",
        "\n",
        "On the down side, to accumulate tokens from this dynamic loop you can't just append them to a python `list`, you need to use a `tf.TensorArray`:\n",
        "\n",
        "```\n",
        "tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n",
        "...\n",
        "for t in tf.range(max_length):\n",
        "    ...\n",
        "    tokens = tokens.write(t, next_token) # next_token shape is (batch, 1)\n",
        "  ...\n",
        "  tokens = tokens.stack()\n",
        "  tokens = einops.rearrange(tokens, 't batch 1 -> batch t')\n",
        "```"
      ],
      "metadata": {
        "id": "FOjQzqUmjbsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This version of the code can be quite a bit more efficient:"
      ],
      "metadata": {
        "id": "Gaga2riKjbwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def translate(self,\n",
        "              texts,\n",
        "              *,\n",
        "              max_length=500,\n",
        "              temperature=tf.constant(0,0)):\n",
        "  shape_checker = ShapeChecker()\n",
        "  context = self.encoder.convert_input(text)\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  shape_checker(conetxt, 'batch s units')\n",
        "\n",
        "  next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "  #initialize the accumulator\n",
        "  tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n",
        "\n",
        "  for t in tf.tange(max_length):\n",
        "    #generate the next token\n",
        "    next_token, done, state = self.decoder.get_next_token(\n",
        "        context, next_token, done, state, temperature\n",
        "    )\n",
        "    shape_checker(next_token, 'batch t1')\n",
        "\n",
        "    #collect the generated tokens\n",
        "    tokens  tokens.write(t, next_token)\n",
        "\n",
        "    #if all the sequences are done, break\n",
        "    if tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  #convert the list of generated token ids to a list of strings\n",
        "  tokens = tokens.stack()\n",
        "  shape_checker(tokens, 't batch t1')\n",
        "  tokens = einops.rearrange(tokens, 't batch 1 -> batch t')\n",
        "  shape_checker(tokens, 'batch t')\n",
        "\n",
        "  text = self.decoder.tokens_to_text(tokens)\n",
        "  shape_checker(text, 'batch')\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "7IKIj36_kO5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With eager execution this implementation performs on par with the original:"
      ],
      "metadata": {
        "id": "xro_Z7IFlcEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result - model.translate(inputs)\n",
        "\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "dGZEQ4mClehF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But when you wrap it in a `tf.function` you'll notice two differences."
      ],
      "metadata": {
        "id": "GI3FfTTjlcIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Export(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string,shape=[None])])\n",
        "  def translate(self, inputs):\n",
        "    return self.model.translate(inputs)"
      ],
      "metadata": {
        "id": "5_d5idLSlmXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export = Export(model)"
      ],
      "metadata": {
        "id": "dbCzTk61l1O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, it's much quicker to trace, since it only creates one copy of the loop body:"
      ],
      "metadata": {
        "id": "jpMfhw2vlcM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "_ = export.translate(inputs)"
      ],
      "metadata": {
        "id": "VnreIoIvl428"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `tf.function` is much faster than running with eager execution, and on small inputs it's often several times faster than the unrolled version, because it can break out of the loop."
      ],
      "metadata": {
        "id": "u4i5NIl8lcQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = export.translate(inputs)\n",
        "\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "Ep300STtmAW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So save this version as well:"
      ],
      "metadata": {
        "id": "7uj62kH0l_dB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tf.sved_model.save(export, 'dynamic_translator',\n",
        "                   signature={'serving_default':export.translate})"
      ],
      "metadata": {
        "id": "hdFEhSnCmIvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "reloaded = tf.saved_model.load('dynamic_translator')\n",
        "_ = reloaded.translate(tf.constant(inputs)) #warmup #dho??warmup인것?"
      ],
      "metadata": {
        "id": "0bhpmA5PmQzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = reloaded.translate(tf.constant(inputs))\n",
        "\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "rQEDfSo8maVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "\n",
        "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
        "* Experiment with training on a larger dataset, or using more epochs.\n",
        "* Try the [transformer tutorial](transformer.ipynb) which implements a similar translation task but uses transformer layers instead of RNNs. This version also uses a `text.BertTokenizer` to implement word-piece tokenization.\n",
        "* Visit the [`tensorflow_addons.seq2seq` tutorial](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt), which demonstrates a higher-level functionality for implementing this sort of sequence-to-sequence model, such as `seq2seq.BeamSearchDecoder`."
      ],
      "metadata": {
        "id": "j-Y_cxvQl_4W"
      }
    }
  ]
}