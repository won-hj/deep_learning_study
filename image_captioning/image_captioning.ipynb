{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNIVRZFuhCxYOX7g99bVkLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/won-hj/deep_learning_study/blob/main/image_captioning/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n"
      ],
      "metadata": {
        "id": "Zh_8aHvRkmJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ig64b5nOoize"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 설치"
      ],
      "metadata": {
        "id": "7kj4cHGN1_2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ],
      "metadata": {
        "id": "ByYiScgIolKU",
        "outputId": "12a3c9cc-db3b-4049-f3eb-b3150dfc783a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --allow-change-held-packages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -y tensorflow estimator keras"
      ],
      "metadata": {
        "id": "tkhYDWvT12dY",
        "outputId": "6ea551ea-53ac-4cde-87c5-9869707a6f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
      ],
      "metadata": {
        "id": "HYyJ1_L315h9",
        "outputId": "057a3acc-05e4-4ce1-e350-32a37b34e7e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.11/dist-packages (2.18.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.11/dist-packages (4.9.7)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (8.1.8)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.9)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (17.0.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (1.16.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.67.1)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.6.0)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_datasets) (25.1.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.66.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "iMQZlwNS1-Ka",
        "outputId": "dfc80d28-d85a-407c-9c82-31b237d0acac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import"
      ],
      "metadata": {
        "id": "1B_gPex22Jb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import concurrent.futures #\n",
        "import collections\n",
        "import dataclasses #\n",
        "import hashlib #\n",
        "import itertools #\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm #\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "502UK9wj2KgF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[선택사항] 데이터처리\n",
        "\n"
      ],
      "metadata": {
        "id": "oa9ahUCJ2z9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터세트 선택\n",
        "Flickr8k 또는 Conceptual Captions 데이터세트의 작은 슬라이스 중 하나.\n",
        "처음부터 다운로드되고 변환되었지만 tensorflow datasets(coco captions 및 전체 conceptual captions)에서 사용할 수 있는 캡션 데이터세트를 사용하기 위해 튜토리얼은 변환하는 것은 어렵지 않음"
      ],
      "metadata": {
        "id": "zDJqJ3PS_to0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Flickr8k**"
      ],
      "metadata": {
        "id": "Lsbo62CN3KKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flickr8k(path='flickr8k'):\n",
        "  path = pathlib.Path(path)\n",
        "\n",
        "  if len(list(path.rglob('*'))) < 16197:\n",
        "    tf.keras.utils.get_file(\n",
        "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
        "        cache_dir='.',\n",
        "        cache_subdir=path,\n",
        "        extract=True)\n",
        "    tf.keras.utils.get_file(\n",
        "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
        "        cache_dir='.',\n",
        "        cache_subdir=path,\n",
        "        extract=True)\n",
        "\n",
        "  captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
        "  captions = (line.split('\\t') for line in captions)\n",
        "  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "\n",
        "  cap_dict = collections.defaultdict(list)\n",
        "  for fname, cap in captions:\n",
        "    cap_dict[fname].append(cap)\n",
        "\n",
        "  train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
        "  train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
        "\n",
        "  test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
        "  test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
        "\n",
        "  train_ds = tf.data.experimental.from_list(train_captions)\n",
        "  test_ds = tf.data.experimental.from_list(test_captions)\n",
        "\n",
        "  return train_ds, test_ds"
      ],
      "metadata": {
        "id": "XM0ZMm9R3JSk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conceptual Captions**"
      ],
      "metadata": {
        "id": "OcnHM-j45LjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conceptual_captions(*, data_dir=\"conceptual_captions\", num_train, num_val):\n",
        "  def iter_index(index_path):\n",
        "    with open(index_path) as f:\n",
        "      for line in f:\n",
        "        caption, url = line.strip().split('\\t')\n",
        "        yield caption, url\n",
        "\n",
        "  def download_image_urls(data_dir, urls):\n",
        "    ex = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
        "    def save_image(url):\n",
        "      hash = hashlib.sha1(url.encode())\n",
        "      # Name the files after the hash of the URL.\n",
        "      file_path = data_dir/f'{hash.hexdigest()}.jpeg'\n",
        "      if file_path.exists():\n",
        "        # Only download each file once.\n",
        "        return file_path\n",
        "\n",
        "      try:\n",
        "        result = requests.get(url, timeout=5)\n",
        "      except Exception:\n",
        "        file_path = None\n",
        "      else:\n",
        "        file_path.write_bytes(result.content)\n",
        "      return file_path\n",
        "\n",
        "    result = []\n",
        "    out_paths = ex.map(save_image, urls)\n",
        "    for file_path in tqdm.tqdm(out_paths, total=len(urls)):\n",
        "      result.append(file_path)\n",
        "\n",
        "    return result\n",
        "\n",
        "  def ds_from_index_file(index_path, data_dir, count):\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "    index = list(itertools.islice(iter_index(index_path), count))\n",
        "    captions = [caption for caption, url in index]\n",
        "    urls = [url for caption, url in index]\n",
        "\n",
        "    paths = download_image_urls(data_dir, urls)\n",
        "\n",
        "    new_captions = []\n",
        "    new_paths = []\n",
        "    for cap, path in zip(captions, paths):\n",
        "      if path is None:\n",
        "        # Download failed, so skip this pair.\n",
        "        continue\n",
        "      new_captions.append(cap)\n",
        "      new_paths.append(path)\n",
        "\n",
        "    new_paths = [str(p) for p in new_paths]\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((new_paths, new_captions))\n",
        "    ds = ds.map(lambda path,cap: (path, cap[tf.newaxis])) # 1 caption per image\n",
        "    return ds\n",
        "\n",
        "  data_dir = pathlib.Path(data_dir)\n",
        "  train_index_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv',\n",
        "    cache_subdir=data_dir,\n",
        "    cache_dir='.')\n",
        "\n",
        "  val_index_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv',\n",
        "    cache_subdir=data_dir,\n",
        "    cache_dir='.')\n",
        "\n",
        "  train_raw = ds_from_index_file(train_index_path, data_dir=data_dir/'train', count=num_train)\n",
        "  test_raw = ds_from_index_file(val_index_path, data_dir=data_dir/'val', count=num_val)\n",
        "\n",
        "  return train_raw, test_raw"
      ],
      "metadata": {
        "id": "9dOJy3Tr5Ogx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **데이터세트 다운로드**\n",
        "\n"
      ],
      "metadata": {
        "id": "JqSQA7LF87Zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flickr8k는 이미지당 5개의 캡션과 더욱 소규모의 다운로드를 위한 더 많은 데이터 포함"
      ],
      "metadata": {
        "id": "htW3E00i9t3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "choose = 'flickr8k1'\n",
        "\n",
        "if choose == 'flickr8k':\n",
        "  train_raw, test_raw = flickr8k()\n",
        "else:\n",
        "  train_raw, test_raw = conceptual_captions(num_train=10000, num_val=5000)"
      ],
      "metadata": {
        "id": "xAJ0Op2w92_9",
        "outputId": "7d63989a-6467-464d-d8f8-7ed627c40ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:35<00:00, 285.12it/s]\n",
            "100%|██████████| 5000/5000 [00:07<00:00, 629.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 두 데이터세트에 대한 로더는 (image_path, captions)쌍을 포함하는 tf.data.Dataset를 반환함\n",
        "* Conceptual Captions 는 이미지당 캡션 1개\n",
        "* Flickr8k 는 이미지당 캡션 5개"
      ],
      "metadata": {
        "id": "CXU_uxxX-Fw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw.element_spec #element_spec 어떻게 알지?"
      ],
      "metadata": {
        "id": "6BHXUSK4-VU5",
        "outputId": "ab5cebaa-1075-4d85-c2ea-188bf8143b3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(1,), dtype=tf.string, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ex_path, ex_captions in train_raw.take(1): #take()\n",
        "  print(ex_path)\n",
        "  print(ex_captions)"
      ],
      "metadata": {
        "id": "kF7Ggmu1-Zaq",
        "outputId": "d9c034e9-56e1-4ded-a177-b242cf583cac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'conceptual_captions/train/fa108ae1f21b05518409b760bfec035dbfb3a194.jpeg', shape=(), dtype=string)\n",
            "tf.Tensor([b'a very typical bus station'], shape=(1,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 특성 추출기\n",
        "\n",
        "각 이미지에서 특성을 추출하기 위해 이미지모델(imagenet 사전 훈련됨)을 사용함\n",
        "모델은 이미지분류기로 훈련되었지만, 'include_top=False'는 최종 분류 레이어 없이 모델을 반환하므로 특성 맵의 최종 레이어를 사용할 수 있음"
      ],
      "metadata": {
        "id": "BewBIkJp-hb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    include_preprocessing=True\n",
        ")\n",
        "mobilenet.trainable=False"
      ],
      "metadata": {
        "id": "4ip7Rlgr-x4I",
        "outputId": "b6eb555b-1264-4e72-83f2-2422fad9d0ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n",
            "\u001b[1m4334752/4334752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음은 모델에 맞게 이미지를 로드하고 크기 조정 하는 함수"
      ],
      "metadata": {
        "id": "BuOrpniI_FJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_jpeg(img, channels=3)\n",
        "  img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "4P3GTtAQ_Isx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델은 입력 매치의 각 이미지에 대한 특성 맵 반환"
      ],
      "metadata": {
        "id": "yKIVeSN9_ULt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_batch = load_image(ex_path)[tf.newaxis, :] #[] 부분; (128, 128, 3) -> (1, 128, 128, 3) 변경/ : >> 원래차원유지\n",
        "\n",
        "print(test_img_batch.shape)\n",
        "print(mobilenet(test_img_batch).shape)"
      ],
      "metadata": {
        "id": "sk0we0Nx_Xg4",
        "outputId": "ce3aac24-b318-42eb-bf59-1e575b19619f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 224, 224, 3)\n",
            "(1, 7, 7, 576)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 토크나이저/벡터라이저 설정\n",
        "\n",
        "[TextVectorization](http://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)레이어를 사용하여 다음 단계에 따라 텍스트 캡션을 정수 시퀀스로 변환\n",
        "\n",
        "- [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt)를 사용하여 모든 캡션을 반복하고 캡션을 단어로 분할하고 상위 단어의 어휘를 계산\n",
        "- 각 단어를 어휘의 인덱스에 매핑하여 모든 캡션을 토큰화, 모든 출력 시퀀스의 길이 50\n",
        "- 단어에서 인덱스로, 인덱스에서 단어로의 메핑을 생성하여 결과 표시"
      ],
      "metadata": {
        "id": "IFRMxbtK_6pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(s):\n",
        "  s = tf.strings.lower(s)\n",
        "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
        "  s = tf.strings.join(['START', s,'END'], separator=' ')\n",
        "  return s"
      ],
      "metadata": {
        "id": "sMsDpgMdAiPF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use the top 5000 words for a vocab\n",
        "vocabulary_size = 5000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=standardize,\n",
        "    ragged=True\n",
        ")\n",
        "#learn the vocabulary from the caption data"
      ],
      "metadata": {
        "id": "ZK9qm2zkA5HJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.adapt(train_raw.map(lambda fp, txt:txt).unbatch().batch(1024)) #전부"
      ],
      "metadata": {
        "id": "CVCJReIaBMIo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "HqTHH4GPBScX",
        "outputId": "b14f7a70-b536-427e-ec70-ff5b86a405d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'START', 'END', 'the', 'a', 'of', 'in', 'on', 'and']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
        "t"
      ],
      "metadata": {
        "id": "mv6XmDmvBWI9",
        "outputId": "2ef90954-f65b-4f9c-c383-021b76eab737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[2, 5, 245, 7, 5, 432, 3], [2, 5, 1502, 120, 3]]>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create mappings for words to indices and indices to words\n",
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token='', #왜지?\n",
        "    vocabulary=tokenizer.get_vocabulary()\n",
        ")\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token='',\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True\n",
        ")"
      ],
      "metadata": {
        "id": "jBMmi-XeBgYY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = index_to_word(t)\n",
        "w.to_list()"
      ],
      "metadata": {
        "id": "4MX-sHzAB2uR",
        "outputId": "e03a6987-96f9-44a5-e6c8-c9ea566da13d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[b'START', b'a', b'cat', b'in', b'a', b'hat', b'END'],\n",
              " [b'START', b'a', b'robot', b'dog', b'END']]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy() #reduce의 의미???단순 내적이라는건가"
      ],
      "metadata": {
        "id": "7WRz39zuB5WD",
        "outputId": "4d434dd3-5ac3-4b76-9d4a-9563ffe98c5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'START a cat in a hat END', b'START a robot dog END'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터세트 준비"
      ],
      "metadata": {
        "id": "1nz0aJ_QCBKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'train_raw' 및 'test_raw' 데이터세트는 1:많은 '(image, captions)' 쌍 포함\n",
        "\n",
        "이 함수는 이미지를 복제하여 캡션에 1:1 매치"
      ],
      "metadata": {
        "id": "dFxmyJCgCFTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_shapes(images, captions):\n",
        "  caption_shape = einops.parse_shape(captions, 'b c') #bc?\n",
        "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
        "  images = einops.repeat(\n",
        "      images, 'b ... -> (b c) ...',\n",
        "      c = caption_shape['c']\n",
        "  )\n",
        "  return images, captions"
      ],
      "metadata": {
        "id": "V-s0NWcxCEkW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
        "  break    #???\n",
        "\n",
        "print('image path:', ex_paths.shape)\n",
        "print('captions: ', ex_captions.shape)\n",
        "print()\n",
        "\n",
        "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
        "\n",
        "print('image_paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)"
      ],
      "metadata": {
        "id": "rAoW_FoYCrQS",
        "outputId": "8edb9dc3-1ab5-417e-a964-5057e3ef30f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image path: (32,)\n",
            "captions:  (32, 1)\n",
            "\n",
            "image_paths: (32,)\n",
            "captions: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "keras 훈련과 호환되려면 데잍터세트느 ㄴ(inputs, labels)쌍을 포함해야함\n",
        "텍스트생성의 경우 토큰은 한 단계 디종된 입력과 라벨\n",
        "이 함수는 '(images, texts)'쌍을 '((images, input_tokens), label_tokens)'쌍으로 변환"
      ],
      "metadata": {
        "id": "aDFZg-lBDFRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_txt(imgs, txts):\n",
        "  tokens = tokenizer(txts)\n",
        "\n",
        "  input_tokens = tokens[..., :-1] #...\n",
        "  label_tokens = tokens[..., 1:]\n",
        "  label_tokens = tf.cast(label_tokens, dtype=tf.int32)\n",
        "  return (imgs, input_tokens), label_tokens\n",
        "\n",
        "def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    # Ensure out_tok is int32\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()"
      ],
      "metadata": {
        "id": "rGGMyMoODTVc"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수는 연산을 데이터세트에 추가\n",
        "1. 이미지 로드(실패이미지무시)\n",
        "2. 이미지를 복제하여 캡션의 숫자와 매칭\n",
        "3. 'image, caption' 쌍을 섞고 재배치\n",
        "4. 텍스트를 토큰화하고 토큰을 이동하여 'label_tokens'을 추가\n",
        "5. 'RaggedTensor'표현에서 텍스트를 패딩처리된 밀도 높은 'Tensor' 표현으로 변환"
      ],
      "metadata": {
        "id": "Xac0gwOkDiGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .shuffle(10000)\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds\n",
        "          .map(match_shapes, tf.data.AUTOTUNE)\n",
        "          .unbatch()\n",
        "          .shuffle(shuffle_buffer)\n",
        "          .batch(batch_size)\n",
        "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "          .map(to_tensor, tf.data.AUTOTUNE)\n",
        "          )"
      ],
      "metadata": {
        "id": "gdGpD3y-D2Mg"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델에 특성 추출기를 설치하고 다음과 같이 데이터세트에서 훈련"
      ],
      "metadata": {
        "id": "07UqLQgGEok9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = prepare_dataset(train_raw, tokenizer)\n",
        "train_ds.element_spec"
      ],
      "metadata": {
        "id": "C-MhtxZdEr0M",
        "outputId": "7556ccb1-0d4d-40bc-a9b4-5e6af335522b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int32, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = prepare_dataset(test_raw, tokenizer)\n",
        "test_ds.element_spec"
      ],
      "metadata": {
        "id": "u1Ia7UMbEwf4",
        "outputId": "03d1a7e1-0c5e-48de-873d-9691a0a123b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int32, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [선택사항] 이미지 특성 캐싱하기\n"
      ],
      "metadata": {
        "id": "8tR24eNiE5xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지 특성 추출기가 변경되지 않으며 이 튜토리얼은 증강을 사용하지 않아 이미지특성은 캐싱될 수 있음\n",
        "<!-- 증강하면 이미지 특성 캐싱안되나? -->\n",
        "텍스트토큰화의경우도 동일\n",
        "\n",
        "캐시를 설정하는ㄴ데 드는 시간은 훈련 및 검증 중 각 에포크에서 다시 획득\n",
        "\n",
        "아래의 코드는 두 개의 함수인 'save_dataset' 및 'load_dataset' 정의"
      ],
      "metadata": {
        "id": "ocIAILVkE-fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):#shards?\n",
        "  #load the images and make batches\n",
        "  ds = (ds\n",
        "        .map(lambda path, caption:(load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  # run the feature extractor on each batch\n",
        "  # dont do this in a .map, because tf.data runs on the CPU\n",
        "  def gen():\n",
        "    for (images, captions) in tqdm.tqdm(ds):\n",
        "      feature_maps = image_model(images)\n",
        "\n",
        "      feature_maps, captions = match_shapes(feature_maps, captions)\n",
        "      yield feature_maps, captions\n",
        "\n",
        "  # wrap the generator in a new tf.data.dataset   #왜?\n",
        "  new_ds = tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=image_model.output_shape),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.string)\n",
        "        )\n",
        "    )\n",
        "  # apply the tokenization\n",
        "  new_ds = (new_ds\n",
        "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "            .unbatch() #shuffle 할때만?\n",
        "            .shuffle(1000))\n",
        "\n",
        "  # save the ds into shard files\n",
        "  def shard_func(i, item):\n",
        "    return i % shards\n",
        "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
        "\n",
        "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
        "  def custom_reader_func(datasets):\n",
        "    datasets = datasets.shuffle(1000)\n",
        "    return datasets.interleave(lambda x: x, cycle_length =cycle_length) #람다 x: 만 하면 x하나만 나오데 되던데 cycle_length같은거 ㄴ왜 돼?\n",
        "\n",
        "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
        "\n",
        "  def drop_index(i, x): #왜?\n",
        "    return x\n",
        "\n",
        "  ds = (ds\n",
        "        .map(drop_index, tf.data.AUTOTUNE)\n",
        "        .shuffle(shuffle)\n",
        "        .padded_batch(batch_size) #\n",
        "        .prefetch(tf.data.AUTOTUNE)) #\n",
        "  return ds"
      ],
      "metadata": {
        "id": "ZjFJKqt_FSws"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dataset(train_raw, 'train_cache', mobilenet, tokenizer)\n",
        "save_dataset(test_raw, 'test_cache', mobilenet, tokenizer)"
      ],
      "metadata": {
        "id": "-96njELBHpGC",
        "outputId": "1ff6be78-a405-4beb-93d4-266a28712bab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "216it [00:55,  3.88it/s]\n",
            "107it [00:30,  3.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 훈련을 위한 데이터 준비\n",
        "\n",
        "이러한 사전 처리 단계 후, 데이터세트는 다음과 같음"
      ],
      "metadata": {
        "id": "Tw7SCj01HxwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = load_dataset('train_cache')\n",
        "test_ds = load_dataset('test_cache')"
      ],
      "metadata": {
        "id": "K9ZM2EK1JVcl"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.element_spec"
      ],
      "metadata": {
        "id": "8KiiOpQ7Jaig",
        "outputId": "18f6617c-c0fc-4d00-dd48-19b87beba4e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 7, 7, 576), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int32, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터세트는 keras 훈련에 적합한 (input, label) 쌍 반환\n",
        "\n",
        "inputs 은 (images, input_tokens) 쌍\n",
        "images 는 특성추출기 모델로 처리됨\n",
        "input_tokens 의 각 위치의 경우 모델은 지금까지의 텍스트를 보고 labels의 같은 위치에서 나열된 다음 텍스트 예측 시도"
      ],
      "metadata": {
        "id": "KYi9SDqsJcoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (inputs, ex_labels) in train_ds.take(1):\n",
        "  (ex_img, ex_in_tok) = inputs\n",
        "\n",
        "  print(ex_img.shape)\n",
        "  print(ex_in_tok.shape)\n",
        "  print(ex_labels.shape)"
      ],
      "metadata": {
        "id": "HGsy3Jg-JuUQ",
        "outputId": "ea93ab1f-c4ae-4dc3-817a-96e04eafc8dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 7, 7, 576)\n",
            "(32, 29)\n",
            "(32, 29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 토큰 및 라벨은 동일하며, 다음과 같이 한 단계민 이동"
      ],
      "metadata": {
        "id": "yVNVa158KAc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ex_in_tok[0].numpy())\n",
        "print(ex_labels[0].numpy())"
      ],
      "metadata": {
        "id": "p0BnZdhQKDpd",
        "outputId": "9d521278-f123-4be4-85a8-c639105c1ddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2 3124 1678    9  102    8    4 1471    7    4  165    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n",
            "[3124 1678    9  102    8    4 1471    7    4  165    3    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 트랜스포머 디코더 모델"
      ],
      "metadata": {
        "id": "FVN4KGXiKKSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 모델은 사전 훈련된 이미지 인코더가 충분하다고 가정하며 텍스트 디코더를 빌드하는데만 집중\n",
        "이 튜토리얼은 2단 레이어 트랜스포머디코더사용\n",
        "<!-- 사전 훈련 이미지 인코더가 충분하지 않다면? 사전 훈련 이미지 인코더를 빌드하는데 집중하려면? 2단 레이어 트랜스포머디코더를 사용하지 않는다면? -->\n",
        "\n",
        "이 구현은 [Transformer 튜토리얼](https://www.tensorflow.org/text/tutorials/transformer)의 구현과 거의 동일\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<th> 트랜스포머 인코더 및 디코더 </th>\n",
        "</tr>\n",
        "<tr>\n",
        "<td> <img width='400' src='https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png'>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "NPo9f_O3KPK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 입력 - 토큰 임베딩 및 위치 인코딩 (seqembedding)\n",
        "2. 디코더 - 각 다음을 포함하는 트랜스포머 디코더 레이어(decoderlayer)의 스택\n",
        "    1. 추후 각 출력 위치가 지금까지 출력에 대해 처리할 수 있는 인과적 셀프어텐션(`causalselfattention`)\n",
        "    2. 각 출력 위치가 입력 이미지를 추리할 수 있는 크로스 어텐션 레이어 (`crossattention`)\n",
        "    3. 각 출력 위치를 독립적으로 추가로 처리하는 피드 포워드 네트워크 (`feedforward`)레이어\n",
        "3. 출력 - 출력 어휘에 대한 멀티 클래스 분류"
      ],
      "metadata": {
        "id": "huQMnExoK56Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 입력"
      ],
      "metadata": {
        "id": "iQasq-ZTL571"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 텍스트는 이미 토큰으로 분할되고 id 시퀀스로 변환되었음\n",
        "CNN 또는 RNN과 다르게 트랜스포머의 어텐션 레이어는 시퀀스의 순서에 대해 변하지 않음\n",
        "몇몇 위치 입력이 없다면 시퀀스가 아닌 세트만 봄\n",
        "따라서 각 토큰 ID에 대한 단순 벡터 임베딩 외 임베딩 레이어는 시퀀스 내 각 위치에 대한 임베딩도 포함\n",
        "\n",
        "`SeqEmbedding` 레이어는 다음과 같이 정의\n",
        "* 각 토큰에 대한 임베딩 벡터 검색\n",
        "* 각 시퀀스 위치에 대한 임베딩 벡터 검색\n",
        "* 두 개 모두 합함\n",
        "* `mask_zero=True`를 이용하여 모델에 대한 keras 마스크 초기화\n",
        "\n",
        "참고: 이 구현은 [Transformer 튜토리얼](https://www.tensorflow.org/text/tutorials/transformer)에서와 같이 고정된 임베딩을 사용하는 대신 위치 임베딩을 학습합니다. 임베딩을 학습하는 것은 코드가 약간 적지만 더 긴 시퀀스로 일반화되지는 않습니다."
      ],
      "metadata": {
        "id": "AnbQmlr8L69z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_length, depth):\n",
        "    super().__init__()\n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim = depth)\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim = vocab_size,\n",
        "        output_dim = depth,\n",
        "        mask_zero=True\n",
        "    )\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, seq):\n",
        "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[1]) # (seq)\n",
        "    x = x[tf.newaxis, :] # (1, seq)\n",
        "    x = self.pos_embedding(x) #(1, seq, depth)\n",
        "\n",
        "    return self.add([seq, x])"
      ],
      "metadata": {
        "id": "nHfkYlr1K6kE"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 디코더"
      ],
      "metadata": {
        "id": "rdMfelOvNkQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더는 표준 트랜스포머 디코더로, 각 세 개의 하위 레이어인 `CausalSelfAttention`, `CrossAttention` 및 `FeedForward`를 포함하는 `DecoderLayers`의 스택을 포함\n",
        "\n",
        "구현은 [Transformer 튜토리얼](https://www.tensorflow.org/text/tutorials/transformer)과 거의 동일하며, 자세한 내용은 이를 참조하세요.\n",
        "<br>\n",
        "\n",
        "\n",
        "다음은 `CausalSelfAttention`레이어"
      ],
      "metadata": {
        "id": "AlzRS4bDNlz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # use add instead pf + so the mask propagates through\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    attn = self.mha(query=x, value=x, use_causal_mask=True)\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "BjUd6kDKN6Lf"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 `CrossAttention`레이어\n",
        "\n",
        "`return_attention_scores` 사용하는 데 유의"
      ],
      "metadata": {
        "id": "90tM2zvaOl5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(\n",
        "        query=x, value=y, return_attention_scores=True)\n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)\n",
        ""
      ],
      "metadata": {
        "id": "BZwSVjhtOrld"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 `FeedFoward` 레이어\n",
        "`layers.Dense`레이어는 입력의 최종 축에 적용된다는 점 기억\n",
        "<!-- 입력의 최종 축? -->\n",
        "입력의 형태는 `(batch, sequence, channels)`이므로 `batch` 및 `sequence` 푹에 걸쳐 포인트별로 자동으로 적용"
      ],
      "metadata": {
        "id": "6OUfSHuxPUB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "    ])\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "ACkeMFKGPmrO"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음으로 이러한 세 가지 레이어를 더 큰 규모의 `DecoderLayer`에 배열\n",
        "각 디코더 레이어는 시퀀스에 세 개의 더 작은 레이어 적용\n",
        "\n",
        "각 하위 레이어 다음의 `outt_seq`형태는 `(batch, sequence, channels)`\n",
        "\n",
        "디코더 레이어는 추후 시각화를 위한 `attention_scores`반환"
      ],
      "metadata": {
        "id": "d3s7G1wRQKhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=units, dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads, key_dim=units, dropout=dropout_rate)\n",
        "    self.ff = FeedFoward(units=units, dropout_rate = dropout_rate)\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    #text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "    otu_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq"
      ],
      "metadata": {
        "id": "gdAQui-LQaZk"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 출력"
      ],
      "metadata": {
        "id": "b-79MxgaRU7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 레이어는 각 위치에서 각 토큰에 대한 로짓 예측을 생성하려면 최소한 `layers.Dense`레이어가 필요"
      ],
      "metadata": {
        "id": "xQ7keMVaRWQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "하지만 이 작업을 좀 더 잘 수행할 수 있도록 추가할 수 있는 몇가지 다른 특성\n",
        "1. **잘못된 토큰 처리**: 모델은 텍스트를 생성합니다. 패딩, 알 수 없는 또는 시작 토큰(`''`, `'[UNK]'`, '[START]'`)을 생성해서는 안됩니다.  따라서 이들에 대한 편향을 큰 음수값으로 설정합니다\n",
        "<!-- 왜? 음수값을 크게하면?? -->\n",
        "    > 참고: 손실 함수의 이러한 토큰 역시 무시해야 합니다.\n",
        "\n",
        "2. **스마트 초기화**: 밀도가 높은 레이어의 기본 초기화는 거의 균일한 확률로 각 토큰을 초기에 예측하는 모델을 제공합니다. 실제 토큰 분포는 균일한 것와는 거리가 멉니다. 출력 레이어의 초기 편향을 위한 최적값은 각 토큰의 확률 로그입니다. 이는 균일한 분포 (`log(vocalbulary_size)`)의 엔트로피로부터의 분포의 한계 엔트로피(`'-p*log(p)'`)로 초기 손실을 줄입니다."
      ],
      "metadata": {
        "id": "TX6NaipJRdaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=tokenizer.vocabulary_size(), **kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "    vocab_dict = {name: id\n",
        "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens.numpy().flatten())\n",
        "\n",
        "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    '''counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0'''\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      # Check if the token is in the vocabulary before accessing its id\n",
        "      if token in vocab_dict:\n",
        "        counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    p = counts_arr/total\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "    entropy = -(log_p*p).sum()\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    # TODO(b/250038731): Fix this.\n",
        "    # An Add layer doesn't work because of the different shapes.\n",
        "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
        "    # the losses.\n",
        "    return x + self.bias"
      ],
      "metadata": {
        "id": "EMb6yGU0Yp45"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "스마트 초기화는 초기 손실을 다음과 같이 상당히 줄임"
      ],
      "metadata": {
        "id": "GB2k1wfxUPDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
      ],
      "metadata": {
        "id": "KJardcKHUR_n",
        "outputId": "9c8a600c-0f8e-4316-eb0a-2400f0e126ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 216/216 [00:02<00:00, 84.45it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Uniform entropy: 8.52\n",
            "Marginal entropy: 6.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 빌드하기"
      ],
      "metadata": {
        "id": "mQo_pRJxUm-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 빌드하려면 몇몇 부분을 조합\n",
        "1. 이미지 `feature_extractor` 및 텍스트 `tokenizer`\n",
        "2. 토큰 ID의 배치를 벡트 `(batch, sequence, channels)`로 변환하기 위한 `seq_embedding`레이어\n",
        "3. 텍스트 및 이미지 데이터를 처리할 `DecoderLayers`레이어의 스택\n",
        "4. 다음 단어가 무엇이어야 하는지에 대한 포인트별 예측을 반환하는 `output_layer`"
      ],
      "metadata": {
        "id": "F5DrLU5CUo2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, feature_extractor, output_layer,\n",
        "               num_layers=1, units=256, max_length = 50,\n",
        "               num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_index = tf.keras.layers.StringLookup(\n",
        "        mask_token='',\n",
        "        vocabulary=tokenizer.get_vocabulary()\n",
        "    )\n",
        "    self.index_to_word = tf.keras.layers.StringLookup(\n",
        "        mask_token='',\n",
        "        vocabulary=tokenizer.get_vocabulary(),\n",
        "        invert=True\n",
        "    )\n",
        "    self.seq_embedding = SeqEmbedding(\n",
        "        vocab_size = tokenizer.vocabulary_size(),\n",
        "        depth = units,\n",
        "        max_length=max_length\n",
        "    )\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)\n",
        "    ]\n",
        "    self.output_layer = output_layer"
      ],
      "metadata": {
        "id": "W5FmOSTWU_Ab"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련을 위해 모델을 호출하면 `image, txt`쌍을 수신\n",
        "이 함수를 더 유용하게 하려면\n",
        "\n",
        " * 이미지에 3개의 채널이 있다면 feature_extrator를 통해 실행, 그렇지 않으면 이미 실행된 것으로 가정\n",
        " * 텍스트에 dtype `tf.string`이 있다면 토크나이저를 통해 실행\n",
        "\n",
        "그런 다음 모델을 실행하는 것은 몇 단계만 수행하면 됨\n",
        "1. 추출된 이미지 특성을 평면화하여 디코더 레이어에 대한 입력이 될 수 있도록 함\n",
        "2. 토큰 임베딩 검색\n",
        "3. 이미지 특성 및 텍스트 임베딩에서 `DecoderLayer`의 스택 실행\n",
        "4. 출력 레이어를 실행하여 각 위치에서 다음 토큰을 예측"
      ],
      "metadata": {
        "id": "Az9g6sHJWHTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  @Captioner.add_method\n",
        "  def call(self, inputs):\n",
        "    image, txt = inputs\n",
        "\n",
        "    if image.shape[-1] == 3:\n",
        "      # Apply the feature-extractor, if you get an RGB image.\n",
        "      image = self.feature_extractor(image)\n",
        "\n",
        "    # Flatten the feature map\n",
        "    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
        "\n",
        "\n",
        "    if txt.dtype == tf.string:\n",
        "      # Apply the tokenizer if you get string inputs.\n",
        "      txt = tokenizer(txt)\n",
        "\n",
        "    txt = self.seq_embedding(txt)\n",
        "\n",
        "    # Look at the image\n",
        "    for dec_layer in self.decoder_layers:\n",
        "      txt = dec_layer(inputs=(image, txt))\n",
        "\n",
        "    txt = self.output_layer(txt)\n",
        "\n",
        "    return txt"
      ],
      "metadata": {
        "id": "wuw2GljEUmGq"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer, units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ],
      "metadata": {
        "id": "xTawW4IsUlsS"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 캡션 생성하기"
      ],
      "metadata": {
        "id": "fGWbd1CiXf5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련을 시작하기 전에, 코드를 약간 작성해 캡션을 생성\n",
        "이를 사용하여 훈련이 어떻게 진행되는지 확인\n",
        "\n",
        "다음과 같이 테스트 이미지 다운로드하여 시작"
      ],
      "metadata": {
        "id": "B_JjkUijXh4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ],
      "metadata": {
        "id": "ocR5sN5eXo5R"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 모델로 이미지를 캡션하려면 다음을 수행\n",
        "\n",
        "* `img_features`추출\n",
        "* `[START]` 토큰으로 출력 토큰 목록 초기화\n",
        "* `img_features` 및 `tokens`를 보델로 전달\n",
        "\n",
        "    * 이는 로짓 목록 반환\n",
        "    * 이로한 로짓을 기반으로 다음 토큰 선택\n",
        "    * 토큰 목록에 이를 추가하고 루프 계속\n",
        "    * `'[END]'` 토큰이 생성되었다면 루프 탈출\n",
        "\n",
        "이를 위해 간단한 메서드 추가"
      ],
      "metadata": {
        "id": "RHzvmtrtX2lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1):\n",
        "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
        "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
        "\n",
        "  tokens = initial # (batch, sequence)\n",
        "  for n in range(50):\n",
        "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
        "    preds = preds[:,-1, :]  #(batch, vocab)\n",
        "    if temperature==0:\n",
        "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
        "    else:\n",
        "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence)\n",
        "\n",
        "    if next[0] == self.word_to_index('[END]'):\n",
        "      break\n",
        "  words = index_to_word(tokens[0, 1:-1])\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  return result.numpy().decode()"
      ],
      "metadata": {
        "id": "n5Sejo4bYRY_"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음은 모델의 훈련되지 않은, 해당 이미지를 위해 생성된 일부 캡션으로 아직 의미가 그다지 없음"
      ],
      "metadata": {
        "id": "HS1Jgke4ZjFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model.simple_gen(image, temperature=t)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "QM_ziIe0ZrfO",
        "outputId": "1c2f471f-5964-433b-d8be-b237a8e9a975",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 2, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'causal_self_attention_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'causal_self_attention_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'captioner_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END the END END END END END END END END END END END END END END END END the END END the END END END END END the END END END END END END END END END END END END END END END END END END END the END\n",
            "END END END the END on END END and with END END the the a END a END END END at END the the of END END END END END END the END the END END in the a in a END person a END END END of a\n",
            "END a many under tattoo END front the a person boat of the running change finish dining tied END END player waking popular company in bottles shoes baseball he illustration crazy view the school to and houses shade book island and END as fits baseball car from person sunny\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "온도 매개변수를 통해 다음 세 모드 사이에 삽입 가능\n",
        "1. 그리디 디코딩(`temperature=0.0`) - 각 단계에서 가장 확률이 높은 다음 토큰 선택\n",
        "2. 로짓(`temperature=1.0`)에 따른 랜덤 샘플링\n",
        "3. 균일 랜덤 샘플링(`temperature >> 1.0`)\n",
        "\n",
        "모델이 훈련되지 않았고 빈도 기반 초기화를 사용하였으므로 '그리디'출력은 (우선)일반적으로 가장 일반적인 토큰인 `['a', '.', '[END]']` 만 포함"
      ],
      "metadata": {
        "id": "pBPawYCcZ3Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 훈련"
      ],
      "metadata": {
        "id": "ek47fJ5DaXub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 훈련하려면 몇몇 추가 컴포넌트 필요\n",
        "* 손실 및 메트릭\n",
        "* 옵티마이저\n",
        "* 선택적 콜백"
      ],
      "metadata": {
        "id": "lJtpdRZGaZBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 손실 및 메트릭"
      ],
      "metadata": {
        "id": "XIZEqLD-albo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음은 마스킹 된 손실 및 정확성에 대한 구현\n",
        "손실에 대한 마스크 계산 시 `loss < 1e8` 주의\n",
        "이 항은 `banned_tokens`에 대한 인적이고 불가능할 정도로 높은 손실을 버림\n",
        "<!-- 무슨 의미? -->"
      ],
      "metadata": {
        "id": "-zyFLGO6ctNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(labels, preds):\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  mask = (labels != 0) & (loss < 1e8)\n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss*mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels!=0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "5lcmthq2c5zJ"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 콜백"
      ],
      "metadata": {
        "id": "EHDjnwu5dowf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 중 피드백을 위해 `keras.callbacks.Callback`을 설정해 각 epoch의 끝에 서퍼 이미지에 대한 일부 캡션 생성"
      ],
      "metadata": {
        "id": "fvGyCAgjdqWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result = self.model.simple_gen(self.image, temperature=t)\n",
        "      print(result)\n",
        "    print()\n",
        "\n",
        "  @property\n",
        "  def model(self):\n",
        "    return self._model\n",
        "  @model.setter\n",
        "  def model(self, value):\n",
        "    # You can add validation here if needed\n",
        "    if not isinstance(value, tf.keras.Model): # Example validation\n",
        "        raise TypeError(\"Model must be a tf.keras.Model instance\")\n",
        "    self._model = value\n"
      ],
      "metadata": {
        "id": "I_5rPqYAdwwr"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이는 첫 번째가 '그리디'인 이전과 같은 이전 예시와 같은 세 개의 출력 문자열을 생성하여 각 단계에서 로짓의 argmax 선택"
      ],
      "metadata": {
        "id": "1pZ0YC62eO3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = GenerateText()\n",
        "g.model = model\n",
        "g.on_epoch_end(0)"
      ],
      "metadata": {
        "id": "ufng1p4seUkf",
        "outputId": "577151d7-075e-4f45-f62c-b9a4aedf81c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "END the END END END END END END END END END END END END END END END END the END END the END END END END END the END END END END END END END END END END END END END END END END END END END the END\n",
            "END END END the of a END in person END END END to the the END END a the END END the END END END the a the END END the with the on END END the END END END the of of the END END END the END\n",
            "in END sign text and palace actor and views of a a via step time illustration applied quarter wet its not dark left premiere the the on are a on vector the ceremony to power gifts building pass man the walking of art view woman for bay END on\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "또한 `callbacks.EarlyStopping`을 사용하여 모델 과적합 방지"
      ],
      "metadata": {
        "id": "mDSXiHQpeYyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks =[\n",
        "    GenerateText(),\n",
        "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "]"
      ],
      "metadata": {
        "id": "ODdpoOiueeQa"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련"
      ],
      "metadata": {
        "id": "NvrMiW_Memu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련을 구성하고 실행"
      ],
      "metadata": {
        "id": "GO335mv3eoOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=masked_loss, metrics=[masked_acc])"
      ],
      "metadata": {
        "id": "0HV7BRLUepcq"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "빈도 보고를 더 많이 하려면, `Dataset.repeat()`메서드 사용, `steps_per_epoch`및`validation_steps`인수를 `Model.fit`으로 설정\n",
        "`Flicker8k`에서 이러한 설정을 통해 데이터세트에 대한 전체 전달은 배치가 900개 이상이지만\n",
        "보고-epoch 아래에는 100 단계가 있음\n",
        "<!-- ???? -->"
      ],
      "metadata": {
        "id": "ATWtGN4We0E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "bU9aI2aUfLAJ",
        "outputId": "a928f49b-a4d9-427e-a64e-569cceccd52d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Value passed to parameter 'labels' has DataType float32 not in list of allowed values: int32, int64",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-182-a474d2464f59>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-177-019ae759a557>\u001b[0m in \u001b[0;36mmasked_loss\u001b[0;34m(labels, preds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmasked_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'labels' has DataType float32 not in list of allowed values: int32, int64"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 실행 동안의 손실 및 정확성 플롯:"
      ],
      "metadata": {
        "id": "eGgx1SeVfXjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylin())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "yihUy3IDfaQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "NO9vpGSifs7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  어텐션 플롯"
      ],
      "metadata": {
        "id": "89zBXxLVfzif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련된 모델을 사용하여 이미지에서 해당 `simple_gen`메서드 실행"
      ],
      "metadata": {
        "id": "qj4tw1LBf2rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.simplt_gen(image, temperature=0.0)\n",
        "result"
      ],
      "metadata": {
        "id": "oITrHbaJf52z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력을 토큰으로 다시 분할"
      ],
      "metadata": {
        "id": "HgPn0vKof9Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str_tokens = result.split()\n",
        "str_tokens.append('[END]')"
      ],
      "metadata": {
        "id": "_hQEh22zf_w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 `DecoderLayers`는 `CrossAttention`레이어에 대한 어텐션 스코어를 캐싱\n",
        "각 어텐션 맵의 형태는 `(batch=1, heads, sequence, image)`"
      ],
      "metadata": {
        "id": "V_-xKRuagIxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_maps = [layer.last_attention_score for layer in model.decoder_layer]\n",
        "[map.shape for map in attn_maps]"
      ],
      "metadata": {
        "id": "GJrw5bNhgSVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "따라서 `image`축을 `height, width`로 다시 분할하는 한편 `batch`축을 따라 맵을 스택한 다음 `(batch, heads)`축에 대해 평균을 냄"
      ],
      "metadata": {
        "id": "Ev0m-yKEgZ3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_maps = tf.concat(attn_maps, axis=0)\n",
        "attention_maps = einops.reduce(\n",
        "    attention_maps,\n",
        "    'batch heads sequence (height, wight) -> sequence height width'.\n",
        "    height =7, width = 7,\n",
        "    reduction='mean'\n",
        ")"
      ],
      "metadata": {
        "id": "PTbWMLyZgjzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 시퀀스 예측을 위한 단일 어텐션 맵이 하나 존재\n",
        "\n",
        "각 맵의 값은 합계가 1"
      ],
      "metadata": {
        "id": "dKDLWPekg4wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
      ],
      "metadata": {
        "id": "waI4ETYxg-Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "따라서 다음은 출력에 대한 각 토큰을 생성하는 동안 모델이 어텐션에 주목"
      ],
      "metadata": {
        "id": "sdHYAxhWhHGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_maps(image, str_tokens, attention_map):\n",
        "  fig = plt.figure(figsize=(16,9))\n",
        "\n",
        "  len_result = len(str_tokens)\n",
        "\n",
        "  title=[]\n",
        "  for i in range(len_result):\n",
        "    map = attention_map[i]\n",
        "    grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "    ax = fig.addsubplot(3, grid_size, i+1)\n",
        "    titles.append(ax.set_title(str_tokens[i]))\n",
        "    img = ax.imshow(image)\n",
        "    ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
        "              clim=[0.0, np.max(map)])\n",
        "\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "xDELMpFqhK1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention_maps(image/255, str_token, attention_maps)"
      ],
      "metadata": {
        "id": "3f0DU9K2h4WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "더 유용한 함수로 함께 통합"
      ],
      "metadata": {
        "id": "QigPIc7Ah-9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Captioner.add_method\n",
        "def run_and_show_attention(self, image, temperature=0.0):\n",
        "  result_txt = self.simple_gen(image, temperature)\n",
        "  str_tokens = result_txt.split()\n",
        "  str_tokens.append('[END]')\n",
        "\n",
        "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
        "  attention_maps = tf.concat(attention_maps, axis=0)\n",
        "  attention_maps = eonops.reduce(\n",
        "      attention_maps,\n",
        "      'batch heads sequence (height, width) -> sequence height width',\n",
        "      height=7, width=7,\n",
        "      reduction='mean')\n",
        "\n",
        "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
        "  t = plt.suptitle(result_txt)\n",
        "  t.set_y(1.05)"
      ],
      "metadata": {
        "id": "TTNGLWHiiBlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_show_attention(model, image)"
      ],
      "metadata": {
        "id": "met51s9SiIAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자체 이미지로 시도해보기\n",
        "\n",
        "재미를 위해 방금 훈련한 모델로 자체 이미지를 캡션하는 데 사용할 수 있는 방법을 제공했습니다. 상대적으로 적은 양의 데이터로 훈련되었으므로 이미지가 훈련 데이터와 다를 수 있습니다(결과가 이상할 수 있습니다!).\n"
      ],
      "metadata": {
        "id": "uR5bddVSi5VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
        "image_path = tf.keras.utils.get_file(origin=image_url)\n",
        "image = load_image(image_path)\n",
        "\n",
        "run_and_show_attention(model, image)"
      ],
      "metadata": {
        "id": "CaKBt7YHjEAN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}