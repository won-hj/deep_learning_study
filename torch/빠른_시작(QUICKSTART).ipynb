{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdOfCceJOIqDWnFW30XEM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/won-hj/deep_learning_study/blob/main/torch/%EB%B9%A0%EB%A5%B8_%EC%8B%9C%EC%9E%91(QUICKSTART).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B907Ft-mUesC"
      },
      "outputs": [],
      "source": [
        "#@빠른 시작(QUICKSTART)\n",
        "#https://tutorials.pytorch.kr/beginner/basics/quickstart_tutorial.html\n",
        "# Google Colab에서 노트북을 실행하실 때에는\n",
        "# https://tutorials.pytorch.kr/beginner/colab 를 참고하세요.\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[파이토치(PyTorch) 기본 익히기](intro.html) \\|\\| **빠른 시작** \\|\\|\n",
        "[텐서(Tensor)](tensorqs_tutorial.html) \\|\\| [Dataset과\n",
        "Dataloader](data_tutorial.html) \\|\\|\n",
        "[변형(Transform)](transforms_tutorial.html) \\|\\| [신경망 모델\n",
        "구성하기](buildmodel_tutorial.html) \\|\\|\n",
        "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
        "[최적화(Optimization)](optimization_tutorial.html) \\|\\| [모델 저장하고\n",
        "불러오기](saveloadrun_tutorial.html)\n",
        "\n",
        "빠른 시작(Quickstart)\n",
        "=====================\n",
        "\n",
        "이번 장에서는 기계 학습의 일반적인 작업들을 위한 API를 통해 실행됩니다.\n",
        "더 자세히 알아보려면 각 장(section)의 링크를 참고하세요.\n",
        "\n",
        "데이터 작업하기\n",
        "---------------\n",
        "\n",
        "파이토치(PyTorch)에는 [데이터 작업을 위한 기본\n",
        "요소](https://pytorch.org/docs/stable/data.html) 두가지인\n",
        "`torch.utils.data.DataLoader` 와 `torch.utils.data.Dataset` 가 있습니다.\n",
        "`Dataset` 은 샘플과 정답(label)을 저장하고, `DataLoader` 는 `Dataset` 을\n",
        "순회 가능한 객체(iterable)로 감쌉니다.\n"
      ],
      "metadata": {
        "id": "4rG11hNYU1A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "uvysmhe-U1li"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch는 [TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html) 및\n",
        "[TorchAudio](https://pytorch.org/audio/stable/index.html) 와 같이 도메인\n",
        "특화 라이브러리를 데이터셋과 함께 제공하고 있습니다. 이 튜토리얼에서는\n",
        "TorchVision 데이터셋을 사용하도록 하겠습니다.\n",
        "\n",
        "`torchvision.datasets` 모듈은 CIFAR, COCO 등과 같은 다양한 실제\n",
        "비전(vision) 데이터에 대한 `Dataset`([전체 목록은\n",
        "여기](https://pytorch.org/vision/stable/datasets.html))을 포함하고\n",
        "있습니다. 이 튜토리얼에서는 FasionMNIST 데이터셋을 사용합니다. 모든\n",
        "TorchVision `Dataset` 은 샘플과 정답을 각각 변경하기 위한 `transform` 과\n",
        "`target_transform` 의 두 인자를 포함합니다.\n"
      ],
      "metadata": {
        "id": "nM4fRYzhVAAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 공개 데이터셋에서 학습 데이터를 내려받습니다.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# 공개 데이터셋에서 테스트 데이터를 내려받습니다.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIG1GvEWVFNe",
        "outputId": "9b9a48cb-bab9-4931-f76a-149774e8032d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 21.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 343kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.23MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 8.87MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```Dataset```을 ```DataLoader```의 인자로 전달합니다. 이는 데이터셋을 순회 가능한 객체(iterable)로 감싸고, 자동화된 배치(batch), 샘플링(sampling), 섞기(shuffle) 및 다중 프로세스로 데이터를 불러오기(multiprocess data loading)를 지원합니다. 여기서는 배치 크기(batch size)를 64로 정의합니다. 즉, 데이터로더(dataloader) 객체의 각 요소는 64개의 특징(feature)과 정답(label)을 묶음(batch)으로 반환합니다."
      ],
      "metadata": {
        "id": "1kLXmHyZVkBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# 데이터로드를 생성합니다.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "  print(f'Shape of X [N, C, H, W]: {X.shape}')\n",
        "  print(f\"Shape of y; {y.shape} {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px_h5MhUWI0E",
        "outputId": "08307462-22df-4963-8d19-d7dc00f4d1df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y; torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[PyTorch에서 데이터를 불러오는 방법](data_tutorial.html) 을 자세히\n",
        "알아보세요.\n"
      ],
      "metadata": {
        "id": "wTHR-a_OWkVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "oWMGPpC3WlRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 만들기\n",
        "\n",
        "PyTorch에서 신경망 모델은 [nn.Module]()을 상속받는 클래스(class)를 생성하여 정의합니다. ```__init__```함수에서 신경망의 계층(layer)들을 정의하고 ```forward```함수에서 신경망에 데이터를 어떻게 전달할지 지정합니다. 가능한 경우 GPU 또는 MPS로 신경망을 이동시켜 연산을 가속(accelerate)합니다."
      ],
      "metadata": {
        "id": "BVeoHwBZWlzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습에 사용할 CPU나 GPU, MPU 장치를 얻습니다.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f'Using {device} device')\n",
        "\n",
        "#모델을 정의합니다.\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10)\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3EA5tUlW6xf",
        "outputId": "cd77bc9c-fc20-4642-96b1-1a85b7b24f66"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\[PyTorch에서 신경망을 정의하는 방법](buildmodel_tutorial.html) 을 자세히\n",
        "알아보세요.\n"
      ],
      "metadata": {
        "id": "9D74yQ9IX3pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "7iKBBkWTX44B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 매개변서 최적화하기\n",
        "===\n",
        "\n",
        "모델을 학습하려면 [손실 함수(loss\n",
        "function)](https://pytorch.org/docs/stable/nn.html#loss-functions) 와\n",
        "[옵티마이저(optimizer)](https://pytorch.org/docs/stable/optim.html) 가\n",
        "필요합니다.\n"
      ],
      "metadata": {
        "id": "OnYHQBn5X5ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "bu7fM9evYEK2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 학습 단계(training loop)에서 모델은 (배치(batch)로 제공되는) 학습 데이터셋에 대한 예측을 수행하고, 예측 오류를 역전파하여 모델의 매개변수를 조정합니다."
      ],
      "metadata": {
        "id": "3LHvjsruYM9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(enumerate(dataloader))\n",
        "print('\\n')\n",
        "print()\n",
        "for batch, (x, Y) in unumerate(dataloader):\n",
        "  break\n",
        "print(batch, x, Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RM33DiRQZHzS",
        "outputId": "e80e03d4-fe7f-44fe-a42a-1281fcec9bfa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-338114644579>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(unumerate(dataloader))\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      #예측 오류 계산\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "\n",
        "      # 역전파\n",
        "      loss.backward()\n",
        "      optimizer.step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        loss, current = loss.item(), (batch+1) * len(X)\n",
        "        print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
        "\n"
      ],
      "metadata": {
        "id": "q6yaufOUYXDh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 학습하고 있는지를 확인하기 위해 테스트 데이터셋으로 모델의 성능을 확인합니다."
      ],
      "metadata": {
        "id": "m2Ig3ESOZvkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "h5c96oDvZ1AF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 단계는 여러번의 반복 단계(에폭(epochs))를 거쳐서 수행됩니다. 각 에폭에서는 모델은 더 나은 예측 하기 위해 매개변수를 학습합니다. 각 에폭마다 모델의 정확도(accuracy)와 손실(loss)을 출력합니다; 에폭마다 정확도가 증가하고 손실이 감소하는 것을 보려고 합니다."
      ],
      "metadata": {
        "id": "kQU1W7YWckIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n--------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kry1x-Wc2wd",
        "outputId": "b5709e46-dcab-4f73-a513-f862622aceb4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "--------------------------------\n",
            "loss: 2.304257  [   64/60000]\n",
            "loss: 2.307563  [ 6464/60000]\n",
            "loss: 2.315438  [12864/60000]\n",
            "loss: 2.310933  [19264/60000]\n",
            "loss: 2.312053  [25664/60000]\n",
            "loss: 2.312326  [32064/60000]\n",
            "loss: 2.306940  [38464/60000]\n",
            "loss: 2.315118  [44864/60000]\n",
            "loss: 2.302119  [51264/60000]\n",
            "loss: 2.309213  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 4.7%, Avg loss: 2.309815 \n",
            "\n",
            "Epoch 2\n",
            "--------------------------------\n",
            "loss: 2.304257  [   64/60000]\n",
            "loss: 2.307563  [ 6464/60000]\n",
            "loss: 2.315438  [12864/60000]\n",
            "loss: 2.310933  [19264/60000]\n",
            "loss: 2.312053  [25664/60000]\n",
            "loss: 2.312326  [32064/60000]\n",
            "loss: 2.306940  [38464/60000]\n",
            "loss: 2.315118  [44864/60000]\n",
            "loss: 2.302119  [51264/60000]\n",
            "loss: 2.309213  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 4.7%, Avg loss: 2.309815 \n",
            "\n",
            "Epoch 3\n",
            "--------------------------------\n",
            "loss: 2.304257  [   64/60000]\n",
            "loss: 2.307563  [ 6464/60000]\n",
            "loss: 2.315438  [12864/60000]\n",
            "loss: 2.310933  [19264/60000]\n",
            "loss: 2.312053  [25664/60000]\n",
            "loss: 2.312326  [32064/60000]\n",
            "loss: 2.306940  [38464/60000]\n",
            "loss: 2.315118  [44864/60000]\n",
            "loss: 2.302119  [51264/60000]\n",
            "loss: 2.309213  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 4.7%, Avg loss: 2.309815 \n",
            "\n",
            "Epoch 4\n",
            "--------------------------------\n",
            "loss: 2.304257  [   64/60000]\n",
            "loss: 2.307563  [ 6464/60000]\n",
            "loss: 2.315438  [12864/60000]\n",
            "loss: 2.310933  [19264/60000]\n",
            "loss: 2.312053  [25664/60000]\n",
            "loss: 2.312326  [32064/60000]\n",
            "loss: 2.306940  [38464/60000]\n",
            "loss: 2.315118  [44864/60000]\n",
            "loss: 2.302119  [51264/60000]\n",
            "loss: 2.309213  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 4.7%, Avg loss: 2.309815 \n",
            "\n",
            "Epoch 5\n",
            "--------------------------------\n",
            "loss: 2.304257  [   64/60000]\n",
            "loss: 2.307563  [ 6464/60000]\n",
            "loss: 2.315438  [12864/60000]\n",
            "loss: 2.310933  [19264/60000]\n",
            "loss: 2.312053  [25664/60000]\n",
            "loss: 2.312326  [32064/60000]\n",
            "loss: 2.306940  [38464/60000]\n",
            "loss: 2.315118  [44864/60000]\n",
            "loss: 2.302119  [51264/60000]\n",
            "loss: 2.309213  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 4.7%, Avg loss: 2.309815 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[모델을 학습하는 방법](optimization_tutorial.html) 을 자세히 알아보세요.\n"
      ],
      "metadata": {
        "id": "s1ejMBG6dFYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 저장하기\n",
        "===\n",
        "\n",
        "모델을 저장하는 일반적인 방법은 (모델의 매개변수들을 포함하여) 내부 상태 사전(internal state dictionary)을 직렬화 (serialize)하는 것입니다."
      ],
      "metadata": {
        "id": "NKIVfm7LdGCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJDz-fWSdQv1",
        "outputId": "8644482f-0638-4206-b7cd-a390680e400f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 불러오기\n",
        "===\n",
        "\n",
        "모델을 불러오는 과정에는 모델 구조를 다시 만들고 상태 사전을 모델에 불러오는 과정이 포함됩니다."
      ],
      "metadata": {
        "id": "QfWBp7XsdYUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR7N-nohdeqf",
        "outputId": "5e9e08c9-5714-4629-d259-ba0650ef5ba6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-b4ef1eff0e4c>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 이 모델을 사용해서 예측을 할 수 있습니다."
      ],
      "metadata": {
        "id": "4si18z2Tdlzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "  x = x.to(device)\n",
        "  pred = model(x)\n",
        "  predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "  print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-NTE2XAdn9q",
        "outputId": "d589c294-7995-466a-8506-0fa9a02684f1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"T-shirt/top\", Actual: \"Ankle boot\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[모델을 저장하고 불러오는 방법](saveloadrun_tutorial.html) 을 자세히\n",
        "알아보세요.\n"
      ],
      "metadata": {
        "id": "KUKrVBr7fw0E"
      }
    }
  ]
}