{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZ4PcMlj7ZPkPOi8xPiY2L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/won-hj/deep_learning_study/blob/main/text_generation/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ],
      "metadata": {
        "id": "M2uXryh9KRE2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_at1DB3m-sZd"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation with an RNN"
      ],
      "metadata": {
        "id": "TV0-e5izKViO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "Note: Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware accelerator > GPU*.\n",
        "\n",
        "This tutorial includes runnable code implemented using [tf.keras](https://www.tensorflow.org/guide/keras/sequential_model) and [eager execution](https://www.tensorflow.org/guide/eager). The following is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt \"Q\":"
      ],
      "metadata": {
        "id": "xegNFsxUKxVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>"
      ],
      "metadata": {
        "id": "gP1-zWNDKmsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure.\n",
        "\n",
        "대부분 문법파괴적인 문장들이고, 모델도 단어의 의미를 학습하진 않았음\n",
        "* 모델은 문자 기반으로 만들어져있어서, 학습이 시작되면 영어 단어가 어떻게 이루어져있는지 혹은 그 단어들이 텍스트를 구성하는 의미인지도 모름\n",
        "* 모델은 화자의 이름이 대문자로 표시된 각본과 같은 구조를 가진 말뭉치 구조의 출력을 만듦\n",
        "* 100자의 작은 묶음으로 학습됐지만 문법, 문맥, 맥락 등 전체적인 흐름이 자연스럽고 더 긴 텍스트 시퀀스를 만들 수 있"
      ],
      "metadata": {
        "id": "dTtfJ92qLScq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 셋업"
      ],
      "metadata": {
        "id": "wNHrZoxXPzKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import TensorFlow and other libraries"
      ],
      "metadata": {
        "id": "zv9hiVi4P3T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "JnKs1Q10P4xL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the Shakespeare dataset\n",
        "\n",
        "Change the following line to run this code on your own data."
      ],
      "metadata": {
        "id": "oFXvD4ccQSaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', origin='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "JDGpjJNVQVVC",
        "outputId": "9de9215f-70d6-4f53-ae54-0a494dfd5b25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ],
      "metadata": {
        "id": "yxayq1m7Qpe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "id": "Bv-BgD6AQrVC",
        "outputId": "c8ab5dd2-1da5-4e2e-dd9c-59d6ef473878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "id": "vjNoO6AOSOfD",
        "outputId": "e377567a-1bdf-4718-a056-51eec30e8039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "id": "zWXntGh2SU2f",
        "outputId": "65ed6fc2-850a-40f2-d225-27727ffda651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process the text"
      ],
      "metadata": {
        "id": "2_go3_4_Shll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation.\n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first.\n",
        "\n",
        "훈련하기 전, 문자열을 숫자로 표현해야 함\n",
        "\n",
        "`tf.keras.layers.StringLookup` 레이어는 문자열과 인덱스 간 매칭을 해줌\n",
        "토큰 단위로 분리될 텍스트만 필요함"
      ],
      "metadata": {
        "id": "hiZc79JOSj0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_text, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "id": "afaySC5wUTb_",
        "outputId": "832bd0b4-bd0e-4033-fabf-0675b949ff51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tf.keras.layers.StringLookup' 레이어:"
      ],
      "metadata": {
        "id": "dq2iIJvPUijo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None,\n",
        ")"
      ],
      "metadata": {
        "id": "f3ScXtnPUw1r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실행하면 토큰들을 character IDs로 변환"
      ],
      "metadata": {
        "id": "jjldDz5DU9uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "id": "PlnYMOPuVIOW",
        "outputId": "993dc6f7-e84b-4b49-9531-f599d0776765",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `tf.keras.layers.StringLookup(..., invert=True)`.  \n",
        "\n",
        "이 튜토리얼은 텍스트를 생성하는게 목표이기 때문에 이렇게 문자열을 벡터로 만들고, 다시 사람이 읽을 수 있도록 문자열로 되돌리는 과정이 중요함\n",
        "`tf.keras.layers.StringLookup(...,invert=True)`를 사"
      ],
      "metadata": {
        "id": "2Fibn00eYZVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way.\n",
        "\n",
        "`sorted(set(text))`로 만든 단어장보단 `tf.keras.layers.StringLoopup`의 `get_vocabulary()`를 사용하는게 `[UNK]`토큰이 일관되게 적용됨"
      ],
      "metadata": {
        "id": "whXtnrAXZfPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "s2Vp6Avbas33"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:\n",
        "\n",
        "인덱스 벡터를 원래 문자로 되돌리는 레이어이며, `tf.RaggedTensor` 형태의 문자들을 반환"
      ],
      "metadata": {
        "id": "Bz---X81bHW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "id": "X7TJDPl2bWud",
        "outputId": "dff89c76-5955-4a47-a89b-5536fa33b7ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can `tf.strings.reduce_join` to join the characters back into strings.\n",
        "\n",
        "`tf.strings.reduce_join`으로도 사용 가능"
      ],
      "metadata": {
        "id": "_imxovZQbcBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "mAyFonBAbkEv",
        "outputId": "832f426b-bbe1-46ed-b567-94dd8e742313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "kQ-UOBk6bonB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The prediction task"
      ],
      "metadata": {
        "id": "sn18y7ltbv0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "하나의 문자나 문자열이 주어질 때 어떤 문자가 다음에 올 수 있을지에 대한 문제가 모델 훈련 시의 목표\n",
        "각 시점마다 다음에 올 문자를 예측하기 위해 시퀀스 현태의 문자들을 모델에 넣어 훈련\n",
        "\n",
        "RNN은 이전 상태를 기억하고 활용하기 때문에 현재까지의 계산 결과를 가지고 다음 상태를 예측"
      ],
      "metadata": {
        "id": "wUn4a0pNb2-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices.\n",
        "\n",
        "텍스트를 시퀀스로 나눔\n",
        "각 입력 시퀀스에는 `seq_length`개의 문자 포함\n",
        "\n",
        "각각의 인풋 시퀀스마다 연관있는 타겟들은 오른쪽으로 한 칸 이동하지 않는 이상 같은 길이를 가짐\n",
        "\n",
        "따라서 텍스트를 `seq_length+1` 이라는 청크들로 쪼갬\n",
        "예를 들어, `seq_length`가 4이고 \"Hello\"라는 텍스트가 있다면 입력 시퀀스는 \"Hell\"이며 타겟 시퀀스는 \"ello\"가 됨\n",
        "\n",
        "이것을 하려면 먼저 `tf.data.Dataset.from_tensor_slices`를 이용해 텍스트 벡터를 문자인덱스 스트림으로 바꿔야 함"
      ],
      "metadata": {
        "id": "0iDWtNqBfWxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "0TDwn9bysgUt",
        "outputId": "0aee5c67-035a-464b-8861-867ce9718a20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "lztN7orcswsT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "oZtkBLg2s0yO",
        "outputId": "78407d10-4268-4f4d-bcd7-d97a3b188270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=100\n",
        "\n"
      ],
      "metadata": {
        "id": "CWAcnIl0s76L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size.\n",
        "\n",
        "`batch`메서드를 사용하면 독립된 문자들을 원하는 크기의 시퀀스로 만드는데 좋음"
      ],
      "metadata": {
        "id": "_mgNV9Igs-h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "lF_YzgpYuEUd",
        "outputId": "f06a7418-ac25-4ada-eba1-4bca47e19949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:\n",
        "\n",
        "토큰들을 스트링으로 다시 합치면 어떻게 되는지 보기 좋음"
      ],
      "metadata": {
        "id": "YrAI-EyjuNUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "id": "HBZIGwz1uZ6u",
        "outputId": "6f1b9843-5fdb-4e6e-b4c3-1bb5e95308bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and\n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:\n",
        "\n",
        "훈련에는 시퀀스 형태의 `(input, label)`쌍이 필요\n",
        "각 시점에서 input은 현재의 문자이고 label은 다음에 올 문자\n",
        "\n",
        "다음은 시퀀스를 입력으로 받아 각 시점마다의 input과 label의 위치를 맞춤"
      ],
      "metadata": {
        "id": "9RjztxOSugJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "x3AjpIHdxFtV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list('Tensorflow'))"
      ],
      "metadata": {
        "id": "LoGdQ12kxPyo",
        "outputId": "ea707bae-4480-4c72-b8d1-1153045ee47e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "u9ySEy81xUju"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_ex, target_ex in dataset.take(1):\n",
        "  print('input: ', text_from_ids(input_ex).numpy())\n",
        "  print('target: ', text_from_ids(target_ex).numpy())"
      ],
      "metadata": {
        "id": "3ggguKzZxYhv",
        "outputId": "f0a6d23c-a0a4-4703-dd12-b6416c730a8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "target:  b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches.\n",
        "\n",
        "`tf.data`로 텍스트를 사용할 시퀀스로 만들었음\n",
        "그러나 모델에 입력하기 전에 섞고 배치로 묶어야 함"
      ],
      "metadata": {
        "id": "62dAB7eKxngc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# buffer size to shuffle\n",
        "'''\n",
        "TensorFlow의 tf.data는 무한한 데이터 스트림도 처리할 수 있어야 해.\n",
        "(예: 끝이 없는 데이터 생성기)\n",
        "그런데 데이터를 한 번에 다 메모리에 올려서 섞으면?\n",
        "→ 메모리 부족으로 속도가 느려지고, 큰 데이터는 아예 불가능!\n",
        "그래서 일정 크기의 버퍼(buffer)를 유지하면서 그 안에서만 랜덤하게 섞어\n",
        "'''\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (dataset\n",
        "           .shuffle(BUFFER_SIZE)\n",
        "           .batch(BATCH_SIZE)\n",
        "           .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "XBGOROqkyTFX",
        "outputId": "4e00cd9c-be35-481a-b6e5-5f6d6ed2a4da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100), dtype=tf.int64, name=None), TensorSpec(shape=(None, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build The Model"
      ],
      "metadata": {
        "id": "jlNfLi5c7nuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)).\n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.\n",
        "\n",
        "* `tf.keras.layers.Embedding`: 입력 계층, 학습 가능한 lookup 테이블, 각 character-ID를 `embedding_dim`차원에 매핑\n",
        "* `tf.keras.layers.GRU`: `units=rnn_units`크기의 RNN(LSTM 사용 가능)\n",
        "* `tf.keras.layers.Dense`: 출력 계층, `vocab_size`개의 출력을 가짐, 각 문자당 하나의 로짓을 출력, 각 문자의 로그우도 점수로 출력\n",
        "\n"
      ],
      "metadata": {
        "id": "68zCcAa67qSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "Y7nufBqpXo7o"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  #@tf.function\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "        #1 불필요한 [0] 사용, 다른 반환값 사\n",
        "        #states = self.gru.get_initial_state(x)[0]# 동적인 batch_size 가져오기\n",
        "        #states = tf.zeros([batch_size, self.gru.units])  # rnn_units 크기로 초기화\n",
        "        #2 반환값 개수 불일치\n",
        "        #states = self.gru.get_initial_state(batch_size=tf.shape(x)[0])\n",
        "        ####\n",
        "        #states = self.gru.get_initial_state(tf.shape(x)[0]) #[0]\n",
        "        batch_size = tf.shape(x)[0]  # 동적 batch_size 가져오기\n",
        "        states = tf.zeros([batch_size, self.gru.units])  # (batch_size, rnn_units)\n",
        "\n",
        "    #r = self.gru(x, initial_state=states, training=training)\n",
        "    #x, states = r[0], r[1:] #\n",
        "    outputs  = self.gru(x, initial_state=states, training=training)\n",
        "    print('gru 출력 수 :  ',len(outputs)) #65\n",
        "    print(f\"Type of outputs: {type(outputs)}\")\n",
        "    print(f\"Length of outputs: {len(outputs)}\")\n",
        "\n",
        "    if isinstance(outputs, tuple) or isinstance(outputs, list):\n",
        "            x = outputs[0]  # 첫 번째 요소 (출력 시퀀스)\n",
        "            states = outputs[1] if len(outputs) > 1 else None  # 두 번째 요소 (최종 상태)\n",
        "    else:\n",
        "            x = outputs  # 단일 텐서일 경우\n",
        "\n",
        "    #x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "YDgjkEN_DGvq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "MIdaig6LD_Cj"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "\n",
        "각 문자(character)에 대해,\n",
        "1️⃣ 모델은 해당 문자의 임베딩을 조회(look up the embedding)하고,\n",
        "2️⃣ 임베딩 벡터를 입력으로 사용해 GRU를 한 타임스텝 실행(runs the GRU one timestep with the embedding as input)한 후,\n",
        "3️⃣ Dense 레이어를 적용하여 다음 문자의 로그-우도를 예측하는 로짓(logits)을 생성(applies the dense layer to generate logits predicting the log-likelihood of the next character)한다.\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ],
      "metadata": {
        "id": "snMkQWJTEJ-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: For training you could use a `keras.Sequential` model here. To  generate text later you'll need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ],
      "metadata": {
        "id": "zp0nltIIFhVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ],
      "metadata": {
        "id": "g5ilYDk8Fo8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape)  # (batch_size, sequence_length, vocab_size)"
      ],
      "metadata": {
        "id": "FSLnE1WVE0Il",
        "outputId": "7ab3528b-090e-4a17-f569-72e06eac4d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gru 출력 수 :   65\n",
            "Type of outputs: <class 'tuple'>\n",
            "Length of outputs: 65\n",
            "(64, 100, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:\n",
        "\n",
        "위의 예시의 경우 seq_length가 100이지만 입력의 길이는 상관없음"
      ],
      "metadata": {
        "id": "G7n_KPvuIweh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "QFRlgQ9OI-hw",
        "outputId": "d8718be9-4118-4a4d-aae6-a87d5c656fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"my_model_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m16,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_9 (\u001b[38;5;33mGRU\u001b[0m)                          │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m64\u001b[0m,      │       \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "│                                      │ \u001b[38;5;34m1024\u001b[0m))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)               │          \u001b[38;5;34m67,650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "│                                      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:\n",
        "\n",
        "실제 예측 결과를 가져오려면 문자 vocab으로 만들어지는 로짓에 의해 정의되는 출력의 분포로부터 샘플링하여 문자 인덱스를 가져와야 함\n",
        "\n",
        "샘플링이 중요한데, 출력 분포에서 argmax를 사용하면 모델이 루프에 빠지귀 쉬"
      ],
      "metadata": {
        "id": "pBVbQsl-JAYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "ZJATRb7WJ-jJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ],
      "metadata": {
        "id": "43QoZXPiJ9-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('input: \\n', text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print('Next Char Predictions:\\n', text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "4k9-cV5HKYOV",
        "outputId": "0c5d7475-5d04-4023-91c4-bc692ebd1a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: \n",
            " b're note: the report of her is\\nextended more than can be thought to begin from such a cottage.\\n\\nPOLIX'\n",
            "\n",
            "Next Char Predictions:\n",
            " b':m,&uZ.lHV$K!jTuCAHclkO;xAroibHpHEUJCF[UNK]KuTqr&\\n&!zELnPYVy;WrKes-OYU;M\\nw qrmaLCkcNhijKESrxNVTYUfe:jlQn'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "GFEuqdvXKpTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "\n",
        "이 시점에서 문제를 일반적인 분류 문제(classification problem) 로 처리할 수 있습니다.\n",
        "이전 RNN 상태와 현재 입력을 기반으로 다음 문자의 클래스를 예측하면 됩니다."
      ],
      "metadata": {
        "id": "_y45bwdJKqhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attach an optimizer, and a loss function"
      ],
      "metadata": {
        "id": "7A30cn5xKs6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n",
        "\n",
        "\n",
        "표준 손실 함수인 `tf.keras.losses.sparse_categorical_crossentropy`를 사용할 수 있습니다.\n",
        "\n",
        "이 함수는 예측값의 마지막 차원(각 문자의 클래스 확률)에서 적용되므로 적절합니다.\n",
        "\n",
        "또한, 모델이 로짓(logits)을 반환하므로 `from_logits=True` 옵션을 설정해야 합니다."
      ],
      "metadata": {
        "id": "0SnlqlM-KtQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "Xp_URzu7QLci"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print('Prediction shape: ', example_batch_predictions.shape, ' # (batch_size, sequence_length, vocab_size)')\n",
        "print('Mean loss:        ', example_batch_mean_loss)"
      ],
      "metadata": {
        "id": "_2dGPkNIQSeM",
        "outputId": "33f05918-f812-4adf-d3c2-fa0b64d97b29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189313, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:\n",
        "\n",
        "새로 초기화된 모델은 자신감이 너무 높으면 안 됩니다.\n",
        "즉, 출력되는 로짓(logits) 값들은 비슷한 크기를 가져야 합니다.\n",
        "\n",
        "이를 확인하는 방법 중 하나는,\n",
        "👉 손실 값의 지수(exponential) 를 계산하면 대략 어휘 크기(vocabulary size) 와 비슷해야 합니다.\n",
        "\n",
        "만약 손실 값이 훨씬 크다면?\n",
        "\n",
        "모델이 틀린 정답에 대해 너무 확신하고 있다는 의미입니다.\n",
        "즉, 잘못된 초기화(bad initialization) 가 이루어졌을 가능성이 큽니다."
      ],
      "metadata": {
        "id": "PQ5aZHqOQEzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "id": "DSNRR-zsQwkm",
        "outputId": "63a932f8-a1f0-43f0-bb78-0b107fd8ef42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.97745"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function.\n",
        "\n"
      ],
      "metadata": {
        "id": "u1KU-2SCQGn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "qw09wLcHQz7k"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure checkpoints"
      ],
      "metadata": {
        "id": "Km7YsKjoQ4tb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:\n",
        "\n",
        "1️⃣ 체크포인트 저장 (Checkpoints)\n",
        "모델 학습 도중 tf.keras.callbacks.ModelCheckpoint 를 사용해 체크포인트를 저장하면,\n",
        "👉 학습이 중간에 중단되더라도 저장된 가중치를 불러와 다시 학습할 수 있음."
      ],
      "metadata": {
        "id": "7F-xG5Z0Q6Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}', '.weights.h5')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "RsTdEvYCR9ws"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute the training"
      ],
      "metadata": {
        "id": "a2vTXCIpQ7ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training.\n",
        "\n",
        "2️⃣ 모델 학습 실행 (Execute Training)\n",
        "Colab을 사용할 경우, GPU 런타임 설정을 하면 학습 속도가 훨씬 빨라짐\n",
        "예제에서는 10 에포크(epoch) 를 사용\n",
        "하지만 더 좋은 성능을 원하면 EPOCHS = 30 처럼 더 오래 학습 가능\n"
      ],
      "metadata": {
        "id": "TwNjj5lEQ9Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "## 계속 문제 생김 아마 튜토리얼 만들어진 때랑 지금이랑 버전차이가 나서 생기는 출력 형식때문에 안맞아서 그런듯"
      ],
      "metadata": {
        "id": "mna85Vr-Tpc-",
        "outputId": "58282e01-2f11-45a3-b7f4-cea3cdad3c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "Exception encountered when calling GRU.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, 100, 256), dtype=float32)\n  • initial_state=tf.Tensor(shape=(None, 1024), dtype=float32)\n  • mask=None\n  • training=True",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-f9296a795ede>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-2888bd0ecd76>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, return_state, training)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#r = self.gru(x, initial_state=states, training=training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#x, states = r[0], r[1:] #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0moutputs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gru 출력 수 :  '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#65\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Type of outputs: {type(outputs)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling GRU.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, 100, 256), dtype=float32)\n  • initial_state=tf.Tensor(shape=(None, 1024), dtype=float32)\n  • mask=None\n  • training=True"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text"
      ],
      "metadata": {
        "id": "s_9w1qwXQ-0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "\n",
        "\n",
        "📌 텍스트 생성 과정 시각화\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n",
        "\n",
        "3️⃣ 텍스트 생성 (Text Generation)\n",
        "기본 원리\n",
        "모델의 출력을 다시 입력으로 사용\n",
        "내부 상태(state)를 유지하면서 반복 실행\n",
        "한 글자씩 예측해 텍스트를 점진적으로 생성"
      ],
      "metadata": {
        "id": "gbsHvPP_Q_lT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following makes a single step prediction:"
      ],
      "metadata": {
        "id": "8va4p3MmRDJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    #create a mask to prevent '[UNK]' from being generated\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        #put a -inf at each bad index\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        #match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    #convert strings to token ids\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    #run the model\n",
        "    #predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,returen_state=True)\n",
        "\n",
        "    #only use the last prediction\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    #apply the prediction mask: prevent '[UNK]' from being generated\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    #sample the output logits to generate token ids\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    #convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    returen predicted_chars, states\n"
      ],
      "metadata": {
        "id": "t4nppnrCT0t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences.\n",
        "\n",
        "4️⃣ 텍스트 생성 방식\n",
        "💡 한 번에 한 글자씩 예측하는 방식\n",
        "\n",
        "초기 문자열과 내부 상태를 모델에 입력\n",
        "다음 글자에 대한 확률 분포(logits) 예측\n",
        "예측된 글자를 입력으로 다시 사용\n",
        "이를 반복하여 새로운 텍스트 생성\n",
        "🔹 한 번 실행하면 한 글자 예측\n",
        "🔹 반복 실행하면 전체 문장 생성 가능"
      ],
      "metadata": {
        "id": "YkOptHyDRFZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states=None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n', '_'*80)\n",
        "print('\\nRun time:', end-start)"
      ],
      "metadata": {
        "id": "8dmI3hPgWDGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.\n",
        "\n",
        "5️⃣ 성능 개선 방법\n",
        "에포크(Epochs) 증가: EPOCHS = 30 으로 학습량 증가\n",
        "시작 문자열(Start String) 변경: 다른 입력 패턴 시도\n",
        "RNN 계층 추가: 모델의 학습 용량 증가\n",
        "온도(Temperature) 조절:\n",
        "낮추면 (temperature < 1.0) → 예측이 더 확정적 (보수적)\n",
        "높이면 (temperature > 1.0) → 더 창의적이지만 랜덤성이 증가"
      ],
      "metadata": {
        "id": "psbjgy4yRGhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above.\n",
        "\n",
        "6️⃣ 더 빠르게 텍스트 생성하는 방법\n",
        "텍스트 생성을 배치(batch)로 수행\n",
        "기존 방식: 한 글자씩 순차적으로 생성 → 느림\n",
        "배치 방식: 한 번에 여러 개의 출력을 생성 → 더 빠름"
      ],
      "metadata": {
        "id": "Ni3_QKFwRHtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states=None\n",
        "next_char = tf.constant(['ROMEO:' 'ROMEO', \"ROMEO\", \"ROMEO\", \"ROMEO\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end=time.time()\n",
        "print(result, '\\n\\n'+'_'*80)\n",
        "print('\\nRun time:', end-start)"
      ],
      "metadata": {
        "id": "Wuoo-_IsWlJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export the generator\n",
        "\n",
        "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted.\n",
        "\n",
        "1️⃣ 모델 내보내기 (Export the Generator)\n",
        "단일 스텝 모델(single-step model) 을 tf.saved_model 형식으로 저장 가능\n",
        "이렇게 하면 다른 환경에서도 모델을 쉽게 불러와 사용 가능\n",
        "👉 관련 문서: SavedModel Guide"
      ],
      "metadata": {
        "id": "5Gsc0wqNRLR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.save_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded=tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "8xsdTiyoYn5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states=None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "YFiFyHDuYwUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but does not give you much control.\n",
        "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement _curriculum  learning_ to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and apply them to the model using the optimizer.\n",
        "\n",
        "\n",
        "2️⃣ 고급: 맞춤형 학습 (Customized Training)\n",
        "위에서 사용한 학습 방식은 \"교사 강제 학습(Teacher Forcing)\"\n",
        "즉, 정답을 직접 입력으로 주기 때문에 모델이 틀렸을 때 복구하는 법을 배우지 못함\n",
        "따라서, 맞춤형 학습 루프를 만들어 모델이 더 강건하게 학습할 수 있도록 개선 가능\n",
        "📌 이런 방식을 적용하는 이유?\n",
        "✅ 오픈 루프(open-loop) 예측 안정화\n",
        "✅ 커리큘럼 학습(curriculum learning) 적용 가능\n",
        "✅ 보다 세밀한 학습 제어 가능\n",
        "\n",
        "3️⃣ 맞춤형 학습 루프(Custom Training Loop)\n",
        "✔ tf.GradientTape 를 사용해 모델 학습 과정을 직접 제어 가능\n",
        "✔ Eager Execution 모드에서 동작하며, 보다 유연한 학습 가능\n",
        "\n",
        "📌 기본 학습 절차\n",
        "1️⃣ 모델 실행 및 손실 계산 (tf.GradientTape 사용)\n",
        "2️⃣ 손실에 대한 그래디언트(Gradient) 계산\n",
        "3️⃣ 옵티마이저를 사용해 모델 업데이트\n",
        "\n",
        "👉 관련 문서: Eager Execution Guide"
      ],
      "metadata": {
        "id": "Juh0yrqcRMhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return {'loss':loss}"
      ],
      "metadata": {
        "id": "ltcpLKalZGhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows you to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods.\n",
        "\n",
        "4️⃣ train_step 함수 사용 방식\n",
        "💡 Keras의 train_step 규칙을 따르는 방법\n",
        "\n",
        "train_step 을 직접 정의해도 Model.compile() 및 Model.fit()을 그대로 사용할 수 있음\n",
        "💡 완전한 맞춤형 학습 루프 작성\n",
        "\n",
        "Model.fit()을 사용하지 않고, 완전히 독립적인 학습 루프를 만들 수도 있음\n",
        "예를 들어, 더 높은 수준의 제어가 필요한 경우 직접 학습 루프를 구현 가능\n",
        "✅ ➡ 따라서, 기본 학습 방법을 활용할 수도 있고, 필요에 따라 완전한 맞춤형 학습 루프를 구현할 수도 있음! 🚀"
      ],
      "metadata": {
        "id": "s4qJhMbARNnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ],
      "metadata": {
        "id": "ha7o2ZKwZhmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "euO3vL2pZshj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "id": "yrH4fNXCZ2gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or if you need more control, you can write your own complete custom training loop:"
      ],
      "metadata": {
        "id": "5bpd1liAROxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 ==0:\n",
        "      template = f'Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}'\n",
        "      print(template)\n",
        "\n",
        "  # saving checkpoint the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "id": "hLKnbYMqZ5sA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}